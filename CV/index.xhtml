<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1 plus MathML 2.0//EN" "http://www.w3.org/Math/DTD/mathml2/xhtml-math11-f.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta name="GENERATOR" content="LyX 2.3.3" />
<meta http-equiv="Content-type" content="text/html;charset=UTF-8" />
<title>Alexander Kain — Curriculum Vitae</title>
<style type='text/css'>
/* Layout-provided Styles */
del.strikeout {
  text-decoration: line-through;
}
h1.title {
font-size: x-large;
margin-bottom: 1ex;
text-align: center;

}
div.standard {
	margin-bottom: 2ex;
}
h2.section {
font-weight: bold;
font-size: x-large;
margin-top: 1.3ex;
margin-bottom: 0.7ex;
text-align: left;

}
h3.subsection_ {
font-weight: bold;
font-size: large;
margin-top: 0.9ex;
margin-bottom: 0.5ex;
text-align: left;

}
div.plain_layout {
text-align: left;

}
ul.itemize {
margin-top: 0.7ex;
margin-bottom: 0.7ex;
margin-left: 3ex;
text-align: left;

}
h3.subsection {
font-weight: bold;
font-size: large;
margin-top: 0.9ex;
margin-bottom: 0.5ex;
text-align: left;

}
h4.subsubsection_ {
font-weight: bold;
font-size: medium;
margin-top: 0.7ex;
margin-bottom: 0.4ex;
text-align: left;

}
ol.enumerate {
margin-top: 0.7ex;
margin-bottom: 0.7ex;
margin-left: 3ex;
text-align: left;

}
ol.enumerate_resume {
margin-top: 0.7ex;
margin-bottom: 0.7ex;
margin-left: 3ex;
text-align: left;

}
h5.paragraph {
font-weight: bold;
font-size: medium;
margin-top: 0.4ex;
text-align: left;

}
h5.paragraph_ {
font-weight: bold;
font-size: medium;
margin-top: 0.4ex;
text-align: left;

}
div.note_comment {
	display: none;
}
span.foot_label {
	vertical-align: super;
	font-size: smaller;
	font-weight: bold;
	text-decoration: underline;
}
div.foot {
	display: inline;
	font-size: small;
	font-weight: medium;
	font-family: serif;
	font-variant: normal;
	font-style: normal;
}
div.foot_inner { display: none; }
div.foot:hover div.foot_inner {
	display: block;
	border: 1px double black;
	margin: 0em 1em;
	padding: 1em;
}


</style>
</head>
<body dir="auto">
<h1 class="title" id='magicparlabel-1'>Alexander Kain — Curriculum Vitae</h1>
<div class="standard" style='text-align: left;' id='magicparlabel-2'><img src='0_Users_kain_lxkain_github_io_CV_self.jpg' alt='image: 0_Users_kain_lxkain_github_io_CV_self.jpg' />

<br />

ORCID 0000-0001-5807-9311
<br />

<a href="https://lxkain.github.io/CV">https://lxkain.github.io/CV</a></div>
<h2 class="section" id='magicparlabel-3'><span class="section_label">1</span> Present Positions</h2>
<h3 class="subsection_" id='magicparlabel-4'>Oregon Health &amp; Science University</h3>
<div class="standard" id='magicparlabel-5'>Associate Professor
<br />

Computer Science &amp; Electrical Engineering (<a href="http://www.ohsu.edu/csee">CSEE</a>)
<br />

Center for Spoken Language Understanding (<a href="http://cslu.ohsu.edu">CSLU</a>)
<br />

Institute on Development and Disability (<a href="http://www.ohsu.edu/xd/research/centers-institutes/institute-on-development-and-disability/">IDD</a>)
<br />

Department of Otolaryngology and Department of Pediatrics
<br />

<div class="note_comment"><div class="plain_layout" id='magicparlabel-9'>Institute on Development &amp; Disability (<a href="http://www.ohsu.edu/xd/research/centers-institutes/institute-on-development-and-disability/index.cfm/reknew">IDD</a>)</div>
</div>School of Medicine (<a href="http://www.ohsu.edu/xd/education/schools/school-of-medicine">SOM</a>)
<br />

Oregon Health &amp; Science University (<a href="http://www.ohsu.edu">OHSU</a>)
<br />

3181 SW Sam Jackson Park Road
<br />

Portland, Oregon 97239-3098
<br />

Email: <a href="kaina@ohsu.edu">kaina@ohsu.edu</a>
<br />

Phone: (503)&nbsp;349-3750</div>
<h3 class="subsection_" id='magicparlabel-14'>BioSpeech, Inc.</h3>
<div class="standard" id='magicparlabel-15'>Chief Technology Innovation Officer (CTIO)
<br />

9946 SW 61st Ave
<br />

Portland, Oregon 97239-3098
<br />

Email: <a href="kain@biospeech.com">kain@biospeech.com</a></div>
<h2 class="section" id='magicparlabel-16'><span class="section_label">2</span> Professional Education</h2>
<h3 class="subsection_" id='magicparlabel-21'>Undergraduate and Graduate</h3>

<ul class="itemize" id='magicparlabel-26'><li class="itemize_item">2001, Ph.&thinsp;D.&nbsp;in Computer Science and Engineering
<br />

<a href="http://www.ogi.edu">Oregon Graduate Institute</a>, Portland, OR</li>
<li class="itemize_item">1995, B.&thinsp;A.&nbsp;in Computer Science and Mathematics
<br />

<a href="http://www.rockford.edu">Rockford College</a>, Rockford, IL</li>
</ul>
<h3 class="subsection_" id='magicparlabel-28'>Postgraduate</h3>

<ul class="itemize" id='magicparlabel-33'><li class="itemize_item">2002–2005, Postdoctoral Training,
<br />

<a href="OGI School of Science &amp; Engineering">OGI School of Science &amp;amp; Engineering</a>, Portland, OR</li>
</ul>
<h2 class="section" id='magicparlabel-47'><span class="section_label">3</span> Professional Experience</h2>
<h3 class="subsection_" id='magicparlabel-48'>Academic</h3>

<ul class="itemize" id='magicparlabel-53'><li class="itemize_item">2014–present, Associate Professor
<br />

2007–2014, Assistant Professor
<br />

2005–2007, Senior Research Associate
<br />

<a href="http://www.ohsu.edu">Oregon Health &amp; Science University</a>, Portland, OR</li>
</ul>
<h3 class="subsection_" id='magicparlabel-62'>Other</h3>

<ul class="itemize" id='magicparlabel-67'><li class="itemize_item">2018–present, Chief Technology Innovation Officer (CTIO)
<br />

2005–2018, Chief Scientist
<br />

<a href="http://www.biospeech.com">BioSpeech, Inc.</a>, Portland, OR</li>
<li class="itemize_item">2001–2008, Lead Speech Synthesis Technologist
<br />

<a href="http://www.sensoryinc.com">Sensory, Inc.</a>, Santa Clara, CA</li>
<li class="itemize_item">1999, Visiting Researcher
<br />

<a href="http://www.research.att.com">AT&amp;T Research Labs</a>, Florham Park, NJ</li>
</ul>
<h2 class="section" id='magicparlabel-70'><span class="section_label">4</span> Scholarship</h2>
<h3 class="subsection" id='magicparlabel-71'><span class="subsection_label">4.1</span> Areas of Research/Scholarly Interest</h3>
<div class="standard" id='magicparlabel-72'>I am interested in <b>innovation</b>, <b>application</b>, and <b>education</b> in computational biomedicine, drawing on many years of <b>machine learning</b> and biological <b>signal processing</b> experience. Recent innovations include:</div>

<ul class="itemize" id='magicparlabel-77'><li class="itemize_item">Representation learning is useful for transferring learned knowledge to tasks for which few or no examples are given but a task representation exists. Specifically, many traditional approaches to high-dimensional regression are deficient in representing the global variance of the targets without additional post-processing. We are exploring how a novel <b>joint autoencoder</b> architecture can address this shortcoming.</li>
<li class="itemize_item">We are experimenting with a novel <b>decomposing autoencoder</b> architecture to decompose parallel data streams into separate aspects such as &ldquo;content&rdquo; vs.&nbsp;&ldquo;style&rdquo;.</li>
</ul>
<div class="standard" id='magicparlabel-79'>Recent applications include:</div>

<ul class="itemize" id='magicparlabel-80'><li class="itemize_item">Phonological disorders affect 10% of preschool and school-age children, adversely affecting their communication, academic performance, and interaction level. Effective pronunciation training requires prolonged supervised practice and interaction. Unfortunately, many children do not have access or only limited access to a speech-language pathologist. <b>Computer-assisted pronunciation training</b> has the potential for being a highly effective teaching aid; however, to-date such systems remain incapable of identifying pronunciation errors with sufficient accuracy. We are experimenting with a phonetic-feature-based speech recognizer that allows for a very detailed classification of speech sounds.</li>
<li class="itemize_item">Speech Intelligibility is the degree to which listeners can understand a speech signal's message. Historically, the specific acoustic sources of intelligibility are poorly understood, and automatic approaches to modify the degree of intelligibility were limited. We invented a hybridization approach that allows for precisely measuring the degree of contribution of one or more acoustic features to speech intelligibility. We applied this approach to find the most relevant acoustic features that cause the intelligibility improvement in clearly-spoken typical and dysarthric speech. This allows a principled study of different remedial strategies. We are also creating algorithms that <b>automatically improve the intelligibility of dysarthric or conversational speech signals</b>, using approaches from speech analysis, machine learning, and speech synthesis. These algorithms may be instrumental for next-generation hearing- and speaking-aids.</li>
<li class="itemize_item"><b>Sleep-disordered breathing</b> is a highly prevalent condition associated with many adverse health problems. As the current means of diagnosis (polysomnography) is obtrusive and ill-suited for mass screening of the population, we explore a minimal-contact, automatic approach that uses acoustics-based methods in conjunction with pulse oximetry.</li>
</ul>
<div class="standard" id='magicparlabel-99'>As an educator in machine learning, I have created the curriculum for, and regularly teach, the course <b>CS627&nbsp;Data Science Programming</b>. I also teach <b>EE 682 Digital Signal Processing</b> and <b>EE 658&nbsp;Speech Signal Processing</b>. My lectures make use of a python-based jupyter-notebook approach.</div>
<h3 class="subsection" id='magicparlabel-100'><span class="subsection_label">4.2</span> Current Collaborators</h3>

<ul class="itemize" id='magicparlabel-101'><li class="itemize_item">Kris Tjaden, Fredrik Van Brenk (University at Buffalo): dysarthria</li>
<li class="itemize_item">Jun Wang (University of Texas, Austin): Silent Speech Interface</li>
<li class="itemize_item">Deanna Britton (PSU/OHSU): dystussia</li>
<li class="itemize_item">Miranda Lim (OHSU): REM Sleep Behavior Disorder</li>
<li class="itemize_item">Jeanne-Marie Guise (OHSU): Epidemiology of Preventable Safety Events</li>
<li class="itemize_item">Marian Dale (OHSU): Speech analysis of patients with progressive supranuclear palsy receiving cerebellar transcranial magnetic stimulation</li>
<li class="itemize_item">Derek Lam and Holden Richards (OHSU): airway obstruction</li>
<li class="itemize_item">Lina Reiss and Nishad Sathe (OHSU): cochlear implants</li>
<li class="itemize_item">Matthew Brodsky and Linda Bryans (OHSU): Speech assessment protocol for Deep Brain Stimulation</li>
<li class="itemize_item">Michelle Molis (VA): speech intelligibility</li>
<li class="itemize_item">Frederick Shic (University of Washington), Tyler Duffield (OHSU), Trevor Hall (OHSU): ASD and Virtual Reality</li>
</ul>
<h3 class="subsection" id='magicparlabel-116'><span class="subsection_label">4.3</span> Grants</h3>
<h4 class="subsubsection_" id='magicparlabel-121'>Planned/Pending</h4>

<ol class="enumerate" id='magicparlabel-122'><li class="enumerate_item">National Institute of Health, &ldquo;Unobtrusive Determination of the location of airway obstructions&rdquo;, PI: Lam (OHSU), pre-submission.</li>
<li class="enumerate_item">National Institute of Health K23, &ldquo;Cerebellar transcranial magnetic stimulation affects speech&rdquo;, PI: Dale (OHSU), pre-submission.</li>
<li class="enumerate_item">National Institute of Health R01, &ldquo;Computer-Assisted Pronunciation Analysis and Training for children with speech sound disorders&rdquo;, PI: Kain (OHSU), pre-submission.</li>
<li class="enumerate_item">National Institute of Health R01, &ldquo;Automatic Detection of REM Sleep Behavior Disorder&rdquo;, PI: Lim (OHSU), pre-submission.</li>
<li class="enumerate_item">National Institute of Health R01, &ldquo;Epidemiology of Preventable Safety Events in pre-hospital EMS for Children&rdquo;, PI: Jeanne-Marie Guise (OHSU), re-submission.</li>
<li class="enumerate_item">National Institute of Health R21, &ldquo;Utility of cough-related airflow measures in determining laryngeal impairment&rdquo;, PI: Britton (OHSU), submitted.</li>
<li class="enumerate_item">National Institute of Health R01, &ldquo;Binaural Spectral Integration with Hearing Loss and Hearing Devices&rdquo;, PI: Reiss (OHSU), starting May 2020.</li>
</ol>
<h4 class="subsubsection_" id='magicparlabel-129'>Current</h4>

<ol class="enumerate" id='magicparlabel-130'><li class="enumerate_item">2019/12/15-2024/06/31: National Institute of Health 1R01DC016621-01A1, &ldquo;Wearable Silent Speech Technology to Enhance Impaired Oral Communication&rdquo;, PI: Jun Wang (University of Texas at Austin). A silent speech interface (SSI) maps articulatory movement data to speech output. Although still in experimental stages, silent speech interfaces hold significant potential for facilitating oral communication in persons after laryngectomy or with other severe voice impairments. Despite the recent efforts on silent speech recognition algorithm development using offline data analysis, online test of SSIs have rarely been conducted. In this paper, we present a preliminary, online test of a real-time, interactive SSI based on electromagnetic motion tracking. The SSI played back synthesized speech sounds in response to the user’s tongue and lip movements. Three English talkers participated in this test, where they mouthed (silently articulated) phrases using the device to complete a phrase-reading task. Among the three participants, 96.67% to 100% of the mouthed phrases were correctly recognized and corresponding synthesized sounds were played after a short delay. Furthermore, one participant demonstrated the feasibility of using the SSI for a short conversation. The experimental results demonstrated the feasibility and potential of silent speech interfaces based on electromagnetic articulography for future clinical applications. My role: I provide signal processing and machine-learning expertise.</li>
<li class="enumerate_item">2015/09/01–2020/08/31: National Institute of Health 2R01DC004689-11A1, &ldquo;Therapeutic Approaches to Dysarthria: Acoustic and Perceptual Correlates&rdquo;, PIs: Tjaden (State University of New York at Buffalo) and <b>Kain</b> (OHSU). 90% of the one million Americans living with idiopathic Parkinson’s disease (PD) and 50% of the 500,000 Americans living with Multiple Sclerosis (MS) will experience dysarthria at some point during the disease. The perceptual sequelae of dysarthria have devastating consequences for quality of life and participation in society by virtue of their effect on social and psychological variables such as employment, leisure activities and relationships. Knowledge of therapy techniques for maximizing perceived speech adequacy, as indexed by intelligibility, therefore is of paramount importance. As a result of our incomplete knowledge of the comparative merits of dysarthria therapy techniques and their variants, however, the choice of a particular technique is not based on a rigorous research base, but is based on either trial and error or the clinician’s educational and experience biases. The proposed project will address these barriers by comparing the acoustic and perceptual consequences of rate reduction, increased vocal intensity and clear speech variants in MS and PD. Our approach is to employ established acoustic measures and perceptual paradigms as well as a state-of-the-art speech re-synthesis technique that will permit conclusions concerning the underlying speech production characteristics, as inferred from the acoustic signal, causing improved intelligibility. Amount: $3.3M.</li>
<li class="enumerate_item">2017/05/01–2020/04/30: National Institute of Health 4R44DC015145-02, &ldquo;SBIR Phase 2: Prosody Assessment Toolbox&rdquo;, PI: Lindaas-Hamilton (BioSpeech). Abnormal receptive or expressive prosody is present in a wide range of disorders, including Autism Spectrum Disorder (ASD), Cognitive Impairment, Down’s syndrome, dysarthria, Parkinson’s disease, depression, schizophrenia, aphasia, Alzheimer’s disease, TBI, Language Impairment, bipolar disorder, ADHD, and PTSD. The characteristics of these prosodic abnormalities and underlying brain dysfunction are still largely unknown, due to the dearth of instruments for assessing prosodic deficits. Building on our broad expertise in computerized prosody assessment, we propose to build a system for researchers that performs automated scoring and acoustic analysis of expressive prosody, allows stimuli to be acoustically modified for detailed perceptual assessment of receptive prosody, and can be extended by researchers to include novel tasks. My role: I provide signal processing and machine-learning expertise, and assist with all scientific aspects of the project. Amount: $706K.</li>
</ol>
<h4 class="subsubsection_" id='magicparlabel-133'>Completed</h4>

<ol class="enumerate_resume" id='magicparlabel-134'><li class="enumerate_resume_item">2015/12/10–2019/04/30: National Institute of Health 5R01DC013996-02, &ldquo;Automatic Voice-Based Assessment of Language Abilities&rdquo;, PI: van Santen (OHSU). Since untreated language disorder can lead to serious behavioral and educational problems, large-scale early language assessment is urgently needed not only for early identification of language disorder but also for planning interventions and tracking progress. However, such large-scale efforts would pose a large burden on professional staff and on other scarce resources. As a result, clinicians, educators, and researchers have argued for the use of computer based assessment. Recently, progress has been made with computer based language assessment, but it has been limited to language comprehension. One contributing factor is that a key technology needed for this, Automatic Speech Recognition (ASR), is perceived as inadequate for accurate scoring of language tests since even the best ASR systems have word error rates in excess of 20%. However, this perception is based on a limited perspective of how ASR can be used for assessment, in which a general-purpose ASR system provides an (often inaccurate) transcript of the child's speech, which then would be scored automatically according to conventional rules. We take an alternative perspective, and propose an innovative approach that comprises two core concepts: (1) creating a special-purpose, test-specific ASR systems whose search space is carefully matched to the space of responses a test may elicit, and (2) integrating these systems with machine-learning based scoring algorithms whereby the latter operate not on the final, best transcript generated by the ASR system, but on the rich layers of intermediate representations that the ASR system computes in the process of recognizing the input speech. My role: (1) Developing automatic voice-based scoring methods for each language test, (2) developing pronunciation screening methods to detect atypical speech, and (3) evaluating the accuracy of automatic voice-based scoring, stopping, and pronunciation screening systems, and comparing TD group with groups with neuro-developmental disorders. Amount: $638K.</li>
<li class="enumerate_resume_item">2014/09/01–2017/08/31: National Institute of Health 1R43MH101978-01A1, &ldquo;System for automatic classification of rodent vocalizations&rdquo;, PI: Lahvis (BioSpeech).<b> </b>Development of treatments for neuropsychiatric disorders presents a formidable challenge. To advance drug discovery, assessments of laboratory rodents are widely employed by academia and industry to model neuropsychiatric disorders. Substantial recent advances in digital recordings of rodent ultrasonic vocalizations (USVs) have engendered interest in assessment of USVs to measure behavior change.<b> </b>A practical obstacle to USV assessment is that they are classified manually.<b> </b>We propose a software system that allows a user to rapidly interrogate recordings of rodent USVs for prosodic content. My role: I provide signal processing and machine-learning expertise, and assist with all scientific aspects of the project. Amount: $324K.</li>
<li class="enumerate_resume_item">2013/12/01–2017/08/31: National Institute of Health <a href="http://projectreporter.nih.gov/project_info_details.cfm?aid=8648375&amp;icde=17801918">1R43DA037588-01A1</a>, &ldquo;Screening for Sleep Disordered Breathing with Minimally Obtrusive Sensors&rdquo;, PI: Snider (BioSpeech). Sleep disordered breathing (SDB) is believed to be a widespread, under-diagnosed condition associated with detrimental health problems, at a high cost to society. The current gold standard for diagnosis of SDB is a time-consuming, expensive, and obtrusive (requiring many attached wires) sleep study, or polysomnography (PSG). The immediate objective of our research is to develop and evaluate a hardware design and a set of algorithms for automatically detecting obstructive, central, or mixed apneas and hypopneas from acoustic, peripheral oxygen saturation (SpO2), and pulse rate data, using an ambient microphone and a wireless pulse oximeter. The long-term goal is to create a low-cost, easy-to-operate, minimally obtrusive, at-home device that can be used for early and frequent screen for SDB in patients' homes, significantly increasing patient comfort while capturing more representative sleep data compared to a clinical sleep study. In collaboration with Chad Hagen, M.&thinsp;D.&nbsp;at the Sleep Disorders Program at OHSU, we aim to (1) develop a screening system by selecting minimally obtrusive sensor hardware and extending state-of-the art algorithms for automatically detecting SDB from acoustic, SpO2, and pulse rate data; (2) collect patient data in the sleep lab and at home from representative populations using the proposed system; (3) determine the screening accuracy by comparing the performance of the proposed system on the collected data against standard PSG-derived clinical results; and (4) measure the usability of an at-home screening device by the target population, by asking subjects who participated in the at-home data collection to complete a survey on various aspects of the setup and operation of the proposed system. My role: I provide signal-processing and machine-learning expertise, and assist with all scientific aspects of the project. Amount: $205K.</li>
<li class="enumerate_resume_item">2016/01/01–2017/04/01: National Institute of Health 1R44DC015145-01, &ldquo;SBIR Phase 1: Prosody Assessment Toolbox&rdquo;, PI: Connors (BioSpeech). Current instruments for assessing prosodic deficits are decades behind those that are used for clinical assessment of other aspects of language. We propose to build a system that addresses these shortcomings. The system performs automated scoring and acoustic analysis of expressive prosody, allows stimuli to be acoustically modified for detailed perceptual assessment of receptive prosody, and can be extended by researchers to include novel tasks. It is evaluated with individuals who have ASD (adults and children), DS (adults and children), or MCI, and a typically developing control group. My role: I provide signal processing and machine-learning expertise, and assist with all scientific aspects of the project. Amount: $1.6M.</li>
<li class="enumerate_resume_item">2010/09/27–2016/09/30: National Science Foundation <a href="http://www.nsf.gov/awardsearch/showAward?AWD_ID=1027834">BCS-1027834</a>, "Computational Models for the Automatic Recognition of Non-Human Primate Social Behaviors", PI: <b>Kain</b> (OHSU). To develop methods that will permit researchers to remotely and automatically monitor behavior of primates and other highly social animals. Amount: $578K.</li>
<li class="enumerate_resume_item">2012/04/01–2015/03/31: National Institute of Health <a href="http://projectreporter.nih.gov/project_info_details.cfm?aid=8249393&amp;icde=17812441">5R44DC009515-03</a>, "SBIR Phase 2: Computer-based auditory skill building program for aural (re)habilitation", PI: Connors (BioSpeech). To extend an adaptive computer-guided software program that focuses on learning phoneme discrimination and identification. See Phase I description. Amount: $400K.</li>
<li class="enumerate_resume_item">2011/12/01–2015/08/31: National Institute of Health <a href="http://projectreporter.nih.gov/project_info_description.cfm?aid=8336853&amp;icde=17803266">R21DC012139</a>, "Computer-Based Pronunciation Analysis for Children with Speech Sound Disorders", PI: <b>Kain</b> (OHSU). In this work we are developing speech-production assessment and pronunciation training tools for children with speech sound disorders. To-date, computer-assisted pronunciation training has not yet been successfully extended to help children with speech sound disorders, primarily because of a lack of accuracy in phoneme-level analysis of the speech signal. My role: I am creating a set of algorithms that will reliably identify and score the intelligibility of a phoneme within an isolated target word, providing immediate, relevant, and understandable feedback about pronunciation errors. The use of human perceptual data during training is an important and new component of the proposed approach. As PI, I am also responsible for overall project supervision and management. Amount: $416K.</li>
<li class="enumerate_resume_item">2010/06/09–2015/05/31: National Science Foundation <a href="http://nsf.gov/awardsearch/showAward?AWD_ID=0964102">IIS-0964102</a>, "Semi-Supervised Discriminative Training of Language Models", PI: <b>Kain</b> (OHSU). To conduct fundamental research in statistical language modeling to improve human language technologies, including automatic speech recognition (ASR) and machine translation (MT). Amount: $519K.</li>
<li class="enumerate_resume_item">2010/05/15–2015/04/30: National Science Foundation <a href="http://www.nsf.gov/awardsearch/showAward.do?AwardNumber=0964468">IIS-0964468</a>, "HCC: Medium: Synthesis and Perception of Speaker Identity", PI: <b>Kain</b> (OHSU). Millions of Americans with impaired or absent speech communication ability rely on Augmentative and Alternative Communication devices with voice output (Speech Generating Devices, or SGDs) to communicate. A psychologically important and desirable feature is the ability to speak with one's own voice, i.&thinsp;e.&nbsp;the ability for the SGD to produce speech that mimics the individual's pre-morbid speech or speech that the individual may be able to intermittently produce. However, current text-to-speech (TTS) systems can only create speech with one or very few supplied speaker characteristics, and cannot be trained to take on the user's voice. My role: Together with Ph.&thinsp;D.&nbsp;students and co-investigators, I am creating a TTS synthesis system that generates speech that sounds like that of a specific individual (Speaker Identity Synthesis, or SIS). In the process we are building and evaluating analysis and synthesis models of the relevant acoustic features, including pitch, duration, and spectrum. Since the system includes a trainability component, this project also involves use of advanced mapping technology in the form of a joint-density Gaussian mixture model. I first proposed this approach in a 1998 publication which has since been cited over 370 times. As PI, I am also responsible for overall project supervision, management, and mentorship of graduate student Mohammadi. Amount: $905K.</li>
<li class="enumerate_resume_item">2011/04/01–2012/03/31: National Institute of Health <a href="http://projectreporter.nih.gov/project_info_details.cfm?aid=8061617&amp;icde=10206352">5R42DC008712</a>, "User Adaptation of AAC Device Voices - Phase 2", PI: Klabbers (BioSpeech). Developing and evaluating voice transformation and prosody modification technologies to customize synthetic voices in AAC devices, mimicking the individual user's pre-morbid speech. See Phase 1 description. Amount: $410K.</li>
<li class="enumerate_resume_item">2011/03/01–2013/03/31: National Institute of Health <a href="http://www.sbir.gov/sbirsearch/detail/368241">1R43DC011706-01</a>, "SBIR Phase 1: Computerized System for Phonemic Awareness Intervention", PI: Connors (BioSpeech). Phonemic awareness, defined as &ldquo;the ability to notice, think about, and work with the individual sounds in spoken words&rdquo;, is considered a necessary skill for literacy. The financial and quality-of-life costs of these impairments are significant, not only because of the link with reading difficulties and hence with future employability, but also because there may exist further links between reading difficulties and a range of psychiatric disorders. This argues for phonemic awareness intervention beyond what can be taught in a regular pre-school or elementary school curriculum. Such intervention is typically provided in the form of one-on-one sessions with a specialized professional (e.&thinsp;g.&nbsp;a Speech Language Pathologist). However, responding to cost concerns and poor access to these services, and also recognizing the importance of frequent intervention sessions, usage of computerized intervention systems is becoming more common. These computerized intervention systems have been steadily improving. However, one significant drawback continues to be their restricted response modalities, typically consisting of the child using a touch screen or a pointing device to select from a set of pictures. By confining the phonemic awareness skills that the system addresses to those that can be tapped into via picture-point-and-click , these systems have a restricted scope of what they can teach. A second drawback of many current systems is that their user interface (e.&thinsp;g.&nbsp;visual layout, tempo) is typically not tunable to the individual characteristics of the child. Given the prevalence of phonemic awareness issues in a broad range of neurodevelopmental disorders, including Autism Spectrum Disorder and Developmental Language Disorder, individual tuning may be critical to address individual neurocognitive weaknesses, such as problems in memory, attention, visual scanning, perceptual motor coordination, and processing speed. We have addressed these drawbacks by (1) taking advantage of drag-and-drop and other touch response modalities that current low-cost touch screen computers are capable of processing and that children are increasingly more familiar with, and (2) by incorporating multiple dimensions of individual tune-ability into the system. My role: Since 2005, I have been the primary developer of the BioSpeech text-to-speech system, a medium-size software project comprised of approximately 10,000 lines of code. For this project, I assisted with integration with the graphical user interface, as well as provided solutions to the problem of synthesizing illegal (i.&thinsp;e.&nbsp;not found in normal use of English) phoneme sequences. Amount: $216K.</li>
<li class="enumerate_resume_item">2009/09/01–2013/08/31: National Science Foundation <a href="http://www.nsf.gov/awardsearch/showAward.do?AwardNumber=0915754">IIS-0915754</a>, "RI: Small: Modeling Coarticulation for Automatic Speech Recognition", PI: <b>Kain</b> (OHSU). We have developed a data-driven, triphone formant trajectory model and methodology for estimating its parameters. In this model, formant targets are speaker dependent, but independent of speaking style. We have validated this model using perceptual listening tests. An analysis of conversationally and clearly spoken speech confirmed that (1) formant trajectories in clear vowels reach their targets more frequently, (2) formants show considerable asynchronicity, and (3) phoneme formant targets approximate their expected values. We also found preliminary evidence that targets derived from clear speech alone perform better at modeling both styles than targets from conversational speech. Having created and validated this model, we are now in the process of applying the approach to disordered speech, paving the way for an objective diagnosis of the degree of coarticulation of dysarthria. Another application is an objective evaluation of the effectiveness of specific speech interventions for certain kinds of dysarthria, e.&thinsp;g.&nbsp;the Lee Silverman Voice Treatment. Finally, this research may also provide an avenue for automatically transforming conversationally-spoken speech to sound as if it had been spoken clearly, thus increasing its intelligibility. A real-time, transparent version of this algorithm would be a desirable feature in many general telecommunications devices. My role: As PI, I am responsible for all aspects of the project, including overall project supervision and management, as well as mentoring of graduate student Bush. Amount: $466.</li>
<li class="enumerate_resume_item">2009/07/15–2012/06/30: National Science Foundation <a href="http://www.nsf.gov/awardsearch/showAward.do?AwardNumber=0905095">IIS-0905095</a>, "HCC: Automatic detection of atypical patterns in cross-modal affect", PI: van Santen (OHSU).The expression of affect in face-to-face situations requires the ability to generate a complex, coordinated, cross-modal affective signal, having gesture, facial expression, vocal prosody, and language content modalities. This ability is compromised in neurological disorders such as Parkinson's disease and autism spectrum disorder (ASD). The long term goal is to build computer-based interactive systems for remediation of poor affect communication and diagnosis of the underlying neurological disorders based on analysis of affective signals. A requirement for such systems is technology to detect atypical patterns in affective signals. We developed a play situation for eliciting affect and collected audio-visual data from approximately 60 children between the ages of 4–7 years old, half of them with ASD and the other half constituting a control group of typically developing children. We labeled the data on relevant affective dimensions, developed algorithms for the analysis of affective incongruity, and then tested the algorithms against the labeled data in order to determine their ability to differentiate between ASD and typical development. My role: I created special <em>delexicalized</em> speech stimuli, using a novel delexicalization algorithm that rendered the lexical content of an utterance unintelligible while preserving important acoustic prosodic cues. Preference tests showed that the proposed method preserved drastically more speaker identity, and sounded more natural than conventional methods. These delexicalized speech stimuli were used in perceptual tests to exclude the effect of lexical content on affect.</li>
<li class="enumerate_resume_item">2009/07/17–2012/06/30: National Institute of Health <a href="http://projectreporter.nih.gov/project_info_details.cfm?aid=7895680&amp;icde=10206236">5R21DC010035</a>, "Quantitative Modeling of Segmental Timing in Dysarthria", PI: van Santen (OHSU). The project seeks to apply a quantitative modeling framework to segment durations in sentences produced by speakers with a variety of neurological diagnoses and dysarthrias. My role: I was responsible for software development for custom recording of speech data and for the extension of my previously published hybridization algorithm for the purposes of creating special perceptual speech stimuli.</li>
<li class="enumerate_resume_item">2008–2009: Nancy Lurie Marks Family Foundation <a href="http://nlmfoundation.org/grants/past_grants/communication_past.html">award</a>, "In Your Own Voice: Personal AAC Voices for Minimally Verbal Children with Autism Spectrum Disorder", PI: van Santen (OHSU). My role: I performed research and development to adapt a text-to-speech voice to sound like a particular child's voice; a task made particularly challenging by the difficulty of extracting reliable acoustic features from children's speech.</li>
<li class="enumerate_resume_item">2007/09/01–2011/08/31: National Science Foundation <a href="http://www.nsf.gov/awardsearch/showAward.do?AwardNumber=0713617">IIS-0713617</a>, "HCC: High-quality Compression, Enhancement, and Personalization of Text-to-Speech Voices", PI: <b>Kain</b> (OHSU). My role: Together with Ph.&thinsp;D.&nbsp;students and co-investigators, I developed text-to-speech (TTS) technologies that focus on elimination of concatenation errors and improved accuracy in the areas of coarticulation, degree of articulation, prosodic effects, and speaker characteristics, using an asynchronous interpolation model that Jan van Santen and I proposed in 2002. These algorithmic advances added to the general acceptability of Speech Generating Devices (SGDs), used by individuals with impaired or absent speech communication.</li>
<li class="enumerate_resume_item">2007/01/01–2008/06/30: National Institute of Health <a href="http://projectreporter.nih.gov/project_info_details.cfm?aid=7219057&amp;icde=10206236">1R41DC008712</a>, "User Adaptation of AAC Device Voices - Phase 1", PI: van Santen (BioSpeech). Speech communication ability is impaired or absent in millions of Americans due to neurological disorders and diseases and to trauma, including autism, Parkinson's disease, and stroke. Augmentative and Alternative Communication (AAC) devices that are operated via switches, keyboards, and a broad range of other input devices, and that have synthetic speech as output, are often the only manner in which these individuals can communicate. A psychologically important feature that no currently available systems have is the ability to speak with the user's voice, i.e., the ability to produce speech that mimics the individual's pre-morbid speech or speech that the individual may be able to intermittently produce. This project used voice transformation (VT) technology to accomplish this goal. My role: I developed and evaluated voice transformation and prosody modification technologies to customize synthetic voices using concatenative speech synthesis technologies, with the aim of mimicking the individual user's pre-morbid speech.</li>
<li class="enumerate_resume_item">2006/09/01–2008/03/31: National Institute of Health <a href="http://projectreporter.nih.gov/project_info_details.cfm?aid=7162050&amp;icde=10206236">1R41DC007240</a>, "Voice Transformation for Dysarthria - Phase 1", PI: van Santen (BioSpeech). Dysarthria is a motor speech disorder due to weakness or poor co- ordination of the speech muscles. Affected muscles include the lungs, larynx, oro- and nasopharynx, soft palate, and articulators (lips, tongue, teeth, and jaw). The degree to which these muscle groups are compromised determines the particular pattern of speech impairment. For example, poor lung function affects the overall volume or loudness, while problems with specific articulators may cause mispronunciations of certain phonemes. There is a great variety of diseases that can cause dysarthria, including Parkinson’s, Multiple Sclerosis, and strokes. My role: I continued development of software that transforms speech compromised by dysarthria into easier-to-understand and more natural-sounding speech. In addition, I designed a hardware configuration that allowed the software to reside on a wearable computer, with a headset microphone as input and powered speaker as output, giving the user full mobility while wearing the speaking-aid.</li>
<li class="enumerate_resume_item">2005/01/10–2010/12/31: National Institute of Health <a href="http://projectreporter.nih.gov/project_info_details.cfm?aid=7546551&amp;icde=10206330">5R01DC007129</a>, "Expressive cross-modal affect integration in Autism", PI: van Santen (OHSU). Autistic Spectrum Disorders (ASD) form a group of neuropsychiatric conditions whose core behavioral features include impairments in reciprocal social interaction, in communication, and repetitive, stereotyped, or restricted interests and behaviors. The importance of prosodic deficits in the adaptive communicative competence of speakers with ASD, as well as for a fuller understanding of the social disabilities central to these disorders is generally recognized; yet current studies are few in number and have significant methodological limitations. The objective of the proposed project is to detail prosodic deficits in young speakers with ASD through a series of experiments that address these disabilities and related areas of function. My role: I developed a delexicalization algorithm that rendered the lexical content of an utterance unintelligible, while preserving important acoustic prosodic cues.</li>
<li class="enumerate_resume_item">2005/01/01–2006/06/30: National Science Foundation <a href="http://www.nsf.gov/awardsearch/showAward.do?AwardNumber=0441125">IIP-0441125</a>, "STTR Phase 1: Small Footprint Speech Synthesis", PI: <b>Kain</b> (BioSpeech). Text-to-speech (TTS) systems have recognized societal benefits for universal access, education, and information access by voice. For example, TTS-based augmentative devices are available for individuals who have lost their voice; and reading machines for the blind have been available for several decades. My role: I developed and implemented a novel algorithm that led to dramatic decreases in disk and memory requirements at a given speech quality level and minimization of the amount of voice recordings needed to create a new synthetic voice. The latter point enabled building personalized TTS systems for individuals with speech disorders who can only intermittently produce normal speech sounds or for individuals who are about to undergo surgery that will irreversibly alter their speech.</li>
<li class="enumerate_resume_item">2001/10/01–2005/09/30: National Science Foundation <a href="http://www.nsf.gov/awardsearch/showAward.do?AwardNumber=0117911">IIS-0117911</a>, "Making Dysarthric Speech Intelligible", PI: van Santen (OHSU). My role: I developed software that transforms speech compromised by dysarthria into easier-to-understand and more natural-sounding speech. The strategy for improving intelligibility is the manipulation of a small set of highly relevant speech features; specifically the energy, pitch, and formant frequencies of an input speech waveform. Pitch and energy are appropriately smoothed, and formant frequencies are mapped with a joint-density Gaussian mixture model, a technique I first introduced in 1998 that since has become the most often used mapping technique in the field. Results from perceptual tests indicated that the transformation improved intelligibility, and that the accompanying removal of the vocal fry improved perceived naturalness.</li>
</ol>
<h3 class="subsection" id='magicparlabel-155'><span class="subsection_label">4.4</span> Publications/Creative Work</h3>
<div class="standard" id='magicparlabel-156'>In the following lists, students or interns under my mentorship are underlined.</div>
<h4 class="subsubsection_" id='magicparlabel-157'>Peer-reviewed Journal Articles and 4–5 page Conference Papers</h4>

<h5 class="paragraph" id='magicparlabel-163'><span class="paragraph_label"></span> In preparation / submitted</h5>

<ul class="itemize" id='magicparlabel-172'><li class="itemize_item">K.&nbsp;Tjaden, A.&nbsp;<b>Kain</b>, G.&nbsp;Wilding, &ldquo;Clear Speech Variants: An Investigation of Intelligibility in Parkinson’s Disease&rdquo;</li>
<li class="itemize_item">A.&nbsp;<b>Kain</b>, T.&nbsp;<u>Dinh</u>, Y.&nbsp;<u>Chen</u>, J.&nbsp;<u>Melanson</u>, K.&nbsp;Tjaden, &ldquo;Intra- and Inter-Speaker Sub-Segmental Duration Conversion&rdquo;</li>
<li class="itemize_item">B.&nbsp;<u>Snider</u>, A.&nbsp;<b>Kain</b>, &ldquo;Automatic Sleep Apnea Detection&rdquo;</li>
<li class="itemize_item"><u>N.&nbsp;Sathe</u>, A.&nbsp;<b>Kain</b>, and L.&nbsp;Reiss, &ldquo;Fusion and Identification of Dichotic Consonants in Normal-Hearing and Hearing-Impaired Listeners&rdquo;</li>
<li class="itemize_item">D.&nbsp;Britton, A.&nbsp;<b>Kain</b>, Y-W.&nbsp;Chen, J.&nbsp;Wiedrick, J.&thinsp;O.&nbsp;Benditt, A.&thinsp;L.&nbsp;Merati, D.&nbsp;Graville, &ldquo;Extreme sawtooth sign in motor neuron disease (MND) suggests laryngeal resistance to forced airflow&rdquo;</li>
<li class="itemize_item">A.&nbsp;<b>Kain</b>, A.&nbsp;<u>Roten</u>, &ldquo;Diacritic-Level Pronunciation Analysis using Phonological Features&rdquo;, ICASSP 2020.</li>
<li class="itemize_item">P.&nbsp;<u>Wallis</u>, D.&nbsp;<u>Yaeger</u>, A.&nbsp;<b>Kain</b>, X.&nbsp;Song, M.&nbsp;Lim, &ldquo;Automatic Event Detection of REM Sleep Without Atonia from Polysomnography Signals using Deep Neural Networks&rdquo;, ICASSP 2020</li>
</ul>
<h5 class="paragraph" id='magicparlabel-183'><span class="paragraph_label"></span> Published</h5>
<h5 class="paragraph" id='magicparlabel-184'><span class="paragraph_label"></span> 2019</h5>

<ol class="enumerate" id='magicparlabel-185'><li class="enumerate_item">F.&nbsp;Van Brenk, K.&nbsp;Tjaden, A.&nbsp;<b>Kain</b>, &ldquo;Identifying Acoustic Correlates of Speaker-Dependent Variation in Slowed Speech Intelligibility: A Hybridization Approach&rdquo;, International Congress of Phonetic Sciences (ICPhS), 2019.</li>
<li class="enumerate_item"><u>D.&nbsp;Tuan</u>, A.&nbsp;<b>Kain</b>, K.&nbsp;Tjaden, &ldquo;Using a Manifold Vocoder for Spectral Voice and Style Conversion&rdquo;, Interspeech, 2019.</li>
</ol>
<h5 class="paragraph" id='magicparlabel-187'><span class="paragraph_label"></span> 2018</h5>

<ol class="enumerate_resume" id='magicparlabel-188'><li class="enumerate_resume_item">S.&nbsp;<u>Dudy</u>, S.&nbsp;Bedrick, M.&nbsp;Asgari, A.<b>&nbsp;Kain</b>, &ldquo;<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5884147/">Automatic Analysis of Pronunciations for Children with Speech Sound Disorders</a>&rdquo;, Computer Speech &amp; Language Journal, 2018.</li>
</ol>
<h5 class="paragraph" id='magicparlabel-189'><span class="paragraph_label"></span> 2017</h5>

<ol class="enumerate_resume" id='magicparlabel-190'><li class="enumerate_resume_item">A.&nbsp;<b>Kain</b>, M.&nbsp;<u>Del Giudice</u>, K.&nbsp;Tjaden, &ldquo;<a href="http://www.isca-speech.org/archive/Interspeech_2017/pdfs/0567.PDF">A Comparison of Sentence-level Speech Intelligibility Metrics</a>&rdquo;, Interspeech, 2017.</li>
<li class="enumerate_resume_item">S.&nbsp;<u>Mohammadi</u>, A.&nbsp;<b>Kain</b>, &ldquo;<a href="http://www.isca-speech.org/archive/Interspeech_2017/pdfs/1434.PDF">Siamese Autoencoders for Speech Style Extraction and Switching Applied to Voice Identification and Conversion</a>&rdquo;, Interspeech, 2017.</li>
<li class="enumerate_resume_item">S.&nbsp;<u>Mohammadi</u>, A.&nbsp;<b>Kain</b>, &ldquo;<a href="http://cslu.ohsu.edu/~kain/pub/Mohammadi2017-SPECOM-Overview.pdf">An Overview of Voice Conversion Systems</a>&rdquo;, Speech Communication, 2017.</li>
</ol>
<h5 class="paragraph" id='magicparlabel-193'><span class="paragraph_label"></span> 2016</h5>

<ol class="enumerate_resume" id='magicparlabel-194'><li class="enumerate_resume_item">S.&nbsp;<u>Mohammadi</u>, A.&nbsp;<b>Kain</b>, &ldquo;<a href="http://cslu.ohsu.edu/~kain/pub/Mohammadi2016-Interspeech-SJAE%20Map.pdf">A Voice Conversion Mapping Function based on a Stacked Joint-Autoencoder</a>&rdquo;, Interspeech, 2016.</li>
<li class="enumerate_resume_item">B.&nbsp;<u>Snider</u> and A.&nbsp;<b>Kain</b>, &ldquo;<a href="http://cslu.ohsu.edu/~kain/pub/Snider2016-ICASSP-ClassificationApnea.pdf">Classification of Respiratory Effort and Disordered Breathing during Sleep from Audio and Pulse Oximetry Signals</a>&rdquo;, ICASSP, 2016.</li>
</ol>
<h5 class="paragraph" id='magicparlabel-196'><span class="paragraph_label"></span> 2015</h5>

<ol class="enumerate_resume" id='magicparlabel-197'><li class="enumerate_resume_item">M.&nbsp;Langarani, J.&nbsp;van Santen, S.&nbsp;<u>Mohammadi</u>, A.&nbsp;<b>Kain</b>, &ldquo;<a href="http://cslu.ohsu.edu/~kain/pub/Langarani2015-Interspeech-Intonation.pdf">Data-driven Foot-based Intonation Generator for Text-to-Speech Synthesis</a>&rdquo;, Interspeech, 2015.</li>
<li class="enumerate_resume_item">S.&nbsp;<u>Mohammadi</u>, A.<b>&nbsp;Kain</b>, &ldquo;<a href="http://cslu.ohsu.edu/~kain/pub/Mohammadi2015-Interspeech-semisupervised.pdf">Semi-supervised Training of a Voice Conversion Mapping Function using a Joint-Autoencoder</a>&rdquo;, Interspeech, 2015.</li>
<li class="enumerate_resume_item">S.&nbsp;<u>Dudy</u>, M.&nbsp;Asgari, and A.&nbsp;<b>Kain</b>, &ldquo;<a href="http://cslu.ohsu.edu/~kain/pub/Dudy2015-EMBC-Pronunciation">Pronunciation Analysis for Children with Speech Sound Disorders</a>&rdquo;, IEEE Engineering in Medicine and Biology society (EMBC), Milan, 2015. (PMC4710861).</li>
</ol>
<h5 class="paragraph_" id='magicparlabel-200'>2014</h5>

<ol class="enumerate_resume" id='magicparlabel-201'><li class="enumerate_resume_item">A.<u>&nbsp;Amano-Kusumoto</u>, J.-P.&nbsp;Hosom, A.&nbsp;<b>Kain</b>, J.&nbsp;Aronoff, &ldquo;<a href="http://authors.elsevier.com/sd/article/S0167639313001684">Determining the relevance of different aspects of formant contours to intelligibility</a>&rdquo;, Speech Communication, vol.&nbsp;59, April 2014.</li>
<li class="enumerate_resume_item">K.&nbsp;Tjaden, A.&nbsp;<b>Kain</b>, J.&nbsp;Lam, &ldquo;<a href="http://jslhr.pubs.asha.org/article.aspx?articleID=1833542">Hybridizing Conversational and Clear Speech to Investigate the Source of Increased Intelligibility in Parkinson’s Disease</a>&rdquo;, Journal of Speech, Language, and Hearing Research, Volume 57, August 2014.</li>
<li class="enumerate_resume_item">S.&nbsp;<u>Mohammadi</u>, A.&nbsp;<b>Kain</b>, &ldquo;<a href="http://cslu.ohsu.edu/~kain/pub/Mohammadi2014-SLT-DNN.pdf">Voice conversion using Deep Neural Networks with speaker-independent pre-training</a>&rdquo;, IEEE Spoken Language Technology Workshop (SLT), 2014.</li>
<li class="enumerate_resume_item">B.&nbsp;<u>Bush</u>, A.&nbsp;<b>Kain</b>, &ldquo;<a href="http://cslu.ohsu.edu/~kain/pub/Bush2014-IS-Cont.pdf">Modeling Coarticulation in Continuous Speech</a>&rdquo;, Interspeech 2014.</li>
</ol>
<h5 class="paragraph_" id='magicparlabel-205'>2013</h5>

<ol class="enumerate_resume" id='magicparlabel-206'><li class="enumerate_resume_item">S.&nbsp;<u>Mohammadi</u>, A.&nbsp;<b>Kain</b>, &ldquo;<a href="http://cslu.ohsu.edu/~kain/pub/Mohammadi2013-ICASSP-Transmutative.pdf">Transmutative Voice Conversion</a>&rdquo;, ICASSP, 2013.</li>
<li class="enumerate_resume_item">B.&nbsp;<u>Bush</u>, A.<b>&nbsp;Kain</b>, &ldquo;<a href="http://cslu.ohsu.edu/~kain/pub/Bush2013-ICASSP-CoarticulationBF.pdf">Estimating Phoneme Formant Targets and Coarticulation Parameters of Conversational and Clear Speech</a>&rdquo;, ICASSP, 2013.</li>
<li class="enumerate_resume_item">B.&nbsp;<u>Snider</u> and A.<b>&nbsp;Kain</b>, &ldquo;<a href="http://cslu.ohsu.edu/~kain/pub/Snider2013-ICASSP-BreathingHMM.pdf">Automatic Classification of Breathing Sounds during Sleep</a>&rdquo;, ICASSP, 2013.</li>
</ol>
<h5 class="paragraph_" id='magicparlabel-209'>2012</h5>

<ol class="enumerate_resume" id='magicparlabel-210'><li class="enumerate_resume_item">S.&nbsp;<u>Mohammadi</u>, A.&nbsp;<b>Kain</b>, J.&nbsp;van Santen, &ldquo;<a href="http://cslu.ohsu.edu/~kain/pub/Mohammadi2012-Interspeech-Clear%20Vowels.pdf">Making Conversational Vowels More Clear</a>&rdquo;, Proceedings of Interspeech, 2012.</li>
<li class="enumerate_resume_item">E.&nbsp;Morley, E.&nbsp;Klabbers, J.&nbsp;van Santen, A.&nbsp;<b>Kain</b>, S.&nbsp;<u>Mohammadi</u>, &ldquo;<a href="http://cslu.ohsu.edu/~kain/pub/Morley2012-Interspeech-F0%20Speaker%20ID.pdf">Synthetic F0 can Effectively Convey Speaker ID in Delexicalized Speech</a>&rdquo;, Interspeech, 2012.</li>
</ol>
<h5 class="paragraph_" id='magicparlabel-212'>2011</h5>

<ol class="enumerate_resume" id='magicparlabel-213'><li class="enumerate_resume_item">E.&nbsp;Morley, J.&nbsp;van Santen, E.&nbsp;Klabbers, A.&nbsp;<b>Kain</b>, &ldquo;<a href="http://cslu.ohsu.edu/~kain/pub/Morley2011-ICASSP-F0.pdf">F0 Range and Peak Alignment across Speakers and Emotions</a>&rdquo;, ICASSP, 2011.</li>
<li class="enumerate_resume_item">B.&nbsp;<u>Bush</u>, J.-P.&nbsp;Hosom, A.&nbsp;<b>Kain</b>, and A.&nbsp;<u>Amano-Kusumoto</u>, &ldquo;<a href="http://cslu.ohsu.edu/~kain/pub/Bush2011-IS-GA.pdf">Using a genetic algorithm to estimate parameters of a coarticulation model</a>&rdquo;, Interspeech, 2011.</li>
</ol>
<h5 class="paragraph_" id='magicparlabel-215'>2010</h5>

<ol class="enumerate_resume" id='magicparlabel-216'><li class="enumerate_resume_item">A.<b>&nbsp;Kain</b> and T.&nbsp;Leen, &ldquo;<a href="http://cslu.ohsu.edu/~kain/pub/Kain2010-SSW7-AIMLSF.pdf">Compression of Line Spectral Frequency Parameters using the Asynchronous Interpolation Model</a>&rdquo;, Proceedings of 7th ISCA Workshop on Speech Synthesis, September 2010.</li>
<li class="enumerate_resume_item">A.<b>&nbsp;Kain</b> and J.&nbsp;van Santen, &ldquo;<a href="http://cslu.ohsu.edu/~kain/pub/Kain2010-IS-Delex.pdf">Frequency-domain delexicalization using surrogate vowels</a>&rdquo;, Interspeech, 2010.</li>
<li class="enumerate_resume_item">A.&nbsp;<u>Amano-Kusumoto</u>, J.-P.&nbsp;Hosom, and A.&nbsp;<b>Kain</b>, &ldquo;<a href="http://cslu.ohsu.edu/~kain/pub/Amano2010-IS-SpeakingStyle.pdf">Speaking style dependency of formant targets</a>&rdquo;, Interspeech, 2010.</li>
<li class="enumerate_resume_item">E.&nbsp;Klabbers, A.<b>&nbsp;Kain</b>, and J.&nbsp;van Santen, &ldquo;<a href="http://cslu.ohsu.edu/~kain/pub/Klabbers2010-IS-SGD.pdf">Evaluation of speaker mimic technology for personalizing SGD voices</a>&rdquo;, Interspeech, 2010.</li>
</ol>
<h5 class="paragraph_" id='magicparlabel-220'>2009</h5>

<ol class="enumerate_resume" id='magicparlabel-221'><li class="enumerate_resume_item">A.<b>&nbsp;Kain</b>, J.&nbsp;van Santen, &ldquo;<a href="http://cslu.ohsu.edu/~kain/pub/Kain2009-ICASSP-Transformation.pdf">Using Speech Transformation to Increase Speech Intelligibility for the Hearing- and Speaking-impaired</a>&rdquo;, Proceedings of ICASSP, April 2009.</li>
<li class="enumerate_resume_item">Q.&nbsp;<u>Miao</u>, A.<b>&nbsp;Kain</b>, J.&nbsp;van Santen, &ldquo;<a href="http://cslu.ohsu.edu/~kain/pub/Miao2009-Interspeech-PerceptualTDXF.pdf">Perceptual Cost Function for Cross-fading Based Concatenation</a>&rdquo;, Proceedings of Interspeech, 2009.</li>
<li class="enumerate_resume_item">R.&nbsp;<u>Moldover</u>, A.<b>&nbsp;Kain</b>, &ldquo;<a href="http://cslu.ohsu.edu/~kain/pub/Moldover2009-ICASSP-LSFAIM">Compression of Line Spectral Frequency Parameters with Asynchronous Interpolation</a>&rdquo;, Proceedings of ICASSP, April 2009.</li>
</ol>
<h5 class="paragraph_" id='magicparlabel-224'>2008</h5>

<ol class="enumerate_resume" id='magicparlabel-225'><li class="enumerate_resume_item">A.<b>&nbsp;Kain</b>, A.&nbsp;<u>Amano-Kusumoto</u>, and J.-P.&nbsp;Hosom, &ldquo;<a href="http://cslu.ohsu.edu/~kain/pub/Kain2008-JASA-Hybridizing.pdf">Hybridizing Conversational and Clear Speech to Determine the Degree of Contribution of Acoustic Features to Intelligibility</a>&rdquo;, Journal of the Acoustical Society of America, vol.&nbsp;124, issue 4, October 2008, pp.&nbsp;2308–2319.</li>
</ol>
<h5 class="paragraph_" id='magicparlabel-226'>2007</h5>

<ol class="enumerate_resume" id='magicparlabel-227'><li class="enumerate_resume_item">A.<b>&nbsp;Kain</b>, J.&nbsp;Hosom, X.<u>&nbsp;Niu</u>, J.&nbsp;van Santen, M.&nbsp;Fried-Oken, J.&nbsp;Staehely, &ldquo;<a href="http://dx.doi.org/10.1016/j.specom.2007.05.001">Improving the Intelligibility of Dysarthric Speech</a>&rdquo;, Speech Communication, vol.&nbsp;49, issue 9, September 2007, pp.&nbsp;743–759.</li>
<li class="enumerate_resume_item">E.&nbsp;Klabbers, J.&nbsp;van Santen, A.<b>&nbsp;Kain</b>, &ldquo;<a href="http://cslu.ohsu.edu/~kain/pub/Klabbers2006-IEEE-Spectral%20Mismatch.pdf">The Contribution of Various Sources of Spectral Mismatch to Audible Discontinuities in a Diphone Database</a>&rdquo;, IEEE Transactions on Audio, Speech, and Language Processing Journal, Volume 15, Issue 3, pp.&nbsp;949–956, March 2007.</li>
<li class="enumerate_resume_item">A.<u>&nbsp;Kusumoto</u>, A.<b>&nbsp;Kain</b>, P.&nbsp;Hosom, and J.&nbsp;van Santen, &ldquo;<a href="http://cslu.ohsu.edu/~kain/pub/Kusumoto2007-Interspeech-Hybridizing.pdf">Hybridizing Conversational and Clear Speech</a>&rdquo;, Proceedings of Interspeech, August 2007.</li>
<li class="enumerate_resume_item">A.<b>&nbsp;Kain</b>, Q.<u>&nbsp;Miao</u>, J.&nbsp;van Santen, &ldquo;<a href="http://cslu.ohsu.edu/~kain/pub/Kain2007-SSW6-Spectral%20Control.pdf">Spectral Control in Concatenative Speech Synthesis</a>&rdquo;, Proceedings of 6th ISCA Workshop on Speech Synthesis, August 2007.</li>
<li class="enumerate_resume_item">A.<b>&nbsp;Kain</b> and J.&nbsp;van Santen, &ldquo;<a href="http://cslu.ohsu.edu/~kain/pub/Kain2007-SSW6-AIM.pdf">Unit-Selection Text-to-Speech Synthesis Using an Asynchronous Interpolation Model</a>&rdquo;, Proceedings of 6th ISCA Workshop on Speech Synthesis, August 2007.</li>
</ol>
<h5 class="paragraph_" id='magicparlabel-232'>2006</h5>

<ol class="enumerate_resume" id='magicparlabel-233'><li class="enumerate_resume_item">X.<u>&nbsp;Niu</u>, A.<b>&nbsp;Kain</b>, J.&nbsp;van Santen, &ldquo;<a href="http://cslu.ohsu.edu/~kain/pub/Niu2006-Interspeech-Velopharyngeal.pdf">A Noninvasive, Low-cost Device to Study the Velopharyngeal Port During Speech and Some Preliminary Results</a>&rdquo;, Proceedings of Interspeech, September 2006.</li>
</ol>
<h5 class="paragraph_" id='magicparlabel-234'>2005</h5>

<ol class="enumerate_resume" id='magicparlabel-235'><li class="enumerate_resume_item">J.&nbsp;van Santen, A.<b>&nbsp;Kain</b>, E.&nbsp;Klabbers, and T.&nbsp;Mishra, &ldquo;<a href="http://cslu.ohsu.edu/~kain/pub/vanSanten2005-SPECOM-Multi-level%20Units.pdf">Synthesis of Prosody using Multi- level Unit Sequences</a>&rdquo;, Speech Communication Journal, vol.&nbsp;46, issues 3–4, pp.&nbsp;365–375, July 2005.</li>
<li class="enumerate_resume_item">X.&nbsp;<u>Niu</u>, A.<b>&nbsp;Kain</b>, J.&nbsp;van Santen, &ldquo;<a href="http://cslu.ohsu.edu/~kain/pub/Niu2005-Eurospeech-Nasal.pdf">Estimation of the Acoustic Properties of the Nasal Tract during the Production of Nasalized Vowels</a>&rdquo;, Proceedings of EUROSPEECH, September 2005.</li>
</ol>
<h5 class="paragraph_" id='magicparlabel-237'>2004</h5>

<ol class="enumerate_resume" id='magicparlabel-238'><li class="enumerate_resume_item">A.<b>&nbsp;Kain</b>, X.&nbsp;<u>Niu</u>, J.&nbsp;Hosom, Q.&nbsp;<u>Miao</u>, J.&nbsp;van Santen, &ldquo;<a href="http://cslu.ohsu.edu/~kain/pub/Kain2004-SSW5-Formant%20Resynthesis.pdf">Formant Re-synthesis of Dysarthric Speech</a>&rdquo;, Proceedings of 5th ISCA Workshop on Speech Synthesis, June 2004.</li>
<li class="enumerate_resume_item">J.&nbsp;van Santen, A.<b>&nbsp;Kain</b>, and E.&nbsp;Klabbers, &ldquo;<a href="http://cslu.ohsu.edu/~kain/pub/vanSanten2004-SpeechProsody-Synthesis%20by%20Recombination.pdf">Synthesis by Recombination of Segmental and Prosodic Information</a>&rdquo;, Speech Prosody 2004, March 2004.</li>
<li class="enumerate_resume_item">H.&nbsp;Duxans, A.&nbsp;Bonafonte, A.<b>&nbsp;Kain</b>, and J.&nbsp;van Santen, &ldquo;<a href="http://cslu.ohsu.edu/~kain/pub/Duxans2004-ICSLP-Dynamic.pdf">Including Dynamic and Phonetic Information in Voice Conversion Systems</a>&rdquo;, Proceedings of ICSLP, October 2004.</li>
</ol>
<h5 class="paragraph_" id='magicparlabel-241'>2003</h5>

<ol class="enumerate_resume" id='magicparlabel-242'><li class="enumerate_resume_item">J.&nbsp;Hosom, A.<b>&nbsp;Kain</b>, T.&nbsp;Mishra, J.&nbsp;van Santen, M.&nbsp;Fried-Oken, J.&nbsp;Staehely, &ldquo;<a href="http://cslu.ohsu.edu/~kain/pub/Hosom2003-ICASSP-Dysarthria.pdf">Intelligibility of modifications to dysarthric speech</a>&rdquo;, Proceedings of ICASSP, May 2003.</li>
<li class="enumerate_resume_item">A.<b>&nbsp;Kain</b> and J.&nbsp;van Santen, &ldquo;<a href="http://cslu.ohsu.edu/~kain/pub/Kain2003-Eurospeech-AIM.pdf">A speech model of acoustic inventories based on asynchronous interpolation</a>&rdquo;, Proceedings of EUROSPEECH, pp.&nbsp;329-332, August 2003.</li>
<li class="enumerate_resume_item">J.&nbsp;van Santen, L.&nbsp;Black, G.&nbsp;Cohen, A.<b>&nbsp;Kain</b>, E.&nbsp;Klabbers, T.&nbsp;Mishra, J.&nbsp;de Villiers, X.<u>&nbsp;Niu</u>, &ldquo;<a href="http://cslu.ohsu.edu/~kain/pub/vanSanten2003-Eurospeech-Expressive.pdf">Applications of computer generated expressive speech for communication disorders</a>&rdquo;, Proceedings of EUROSPEECH, pp.&nbsp;1657-1660, August 2003.</li>
</ol>
<h5 class="paragraph_" id='magicparlabel-245'>2002</h5>

<ol class="enumerate_resume" id='magicparlabel-246'><li class="enumerate_resume_item">A.<b>&nbsp;Kain</b> and J.&nbsp;van Santen, &ldquo;<a href="http://cslu.ohsu.edu/~kain/pub/Kain2002-IEEE-AIM.pdf">Compression of Acoustic Inventories using Asynchronous Interpolation</a>&rdquo;, Proceedings of IEEE Workshop on Speech Synthesis, pp.&nbsp;83-86, September 2002.</li>
<li class="enumerate_resume_item">J.&nbsp;van Santen, J.&nbsp;Wouters, and A.<b>&nbsp;Kain</b>, &ldquo;<a href="http://cslu.ohsu.edu/~kain/pub/vanSanten2002-IEEE-Tribute.pdf">Modification of Speech: A Tribute to Mike Macon</a>&rdquo;, Proceedings of IEEE Workshop on Speech Synthesis, September 2002.</li>
</ol>
<h5 class="paragraph_" id='magicparlabel-248'>2001</h5>

<ol class="enumerate_resume" id='magicparlabel-249'><li class="enumerate_resume_item">A.<b>&nbsp;Kain</b> and M.&nbsp;Macon, &ldquo;<a href="http://cslu.ohsu.edu/~kain/pub/Kain2001-ICASSP-Residual%20Prediction.pdf">Design and Evaluation of a Voice Conversion Algorithm based on Spectral Envelope Mapping and Residual Prediction</a>&rdquo;, Proceedings of ICASSP, May 2001.</li>
</ol>
<h5 class="paragraph_" id='magicparlabel-250'>2000 and earlier</h5>

<ol class="enumerate_resume" id='magicparlabel-251'><li class="enumerate_resume_item">A.<b>&nbsp;Kain</b> and Y.&nbsp;Stylianou, &ldquo;<a href="http://cslu.ohsu.edu/~kain/pub/Kain2000-ICASSP-Spectral%20Pitch.pdf">Stochastic Modeling of Spectral Adjustment for High Quality Pitch Modification</a>&rdquo;, Proceedings of ICASSP, June 2000, vol.&nbsp;2, pp.&nbsp;949–952.</li>
<li class="enumerate_resume_item">J.&nbsp;House, A.<b>&nbsp;Kain</b>, and J.&nbsp;Hines, &ldquo;<a href="http://cslu.ohsu.edu/~kain/pub/House2000-GECCO-ESP.pdf">ESP - Metaphor for learning: an evolutionary algorithm</a>&rdquo;, Proceedings of GECCO 2000, Las Vegas, NV.</li>
<li class="enumerate_resume_item">A.<b>&nbsp;Kain</b> and M.&nbsp;Macon, &ldquo;<a href="http://cslu.ohsu.edu/~kain/pub/Kain1998-SSW3-Personalizing.pdf">Personalizing a speech synthesizer by voice adaptation</a>&rdquo;, Third ESCA / COCOSDA International Speech Synthesis Workshop, November 1998, pp.&nbsp;225–230.</li>
<li class="enumerate_resume_item">A.<b>&nbsp;Kain</b> and M.&nbsp;Macon, &ldquo;<a href="http://cslu.ohsu.edu/~kain/pub/Kain1998-ICSLP-Sparse.pdf">Text-to-speech voice adaptation from sparse training data</a>&rdquo;, Proceedings of ICSLP, November 1998, vol.&nbsp;7, pp.&nbsp;2847–50.</li>
<li class="enumerate_resume_item">A.<b>&nbsp;Kain</b> and M.&nbsp;Macon, &ldquo;<a href="http://cslu.ohsu.edu/~kain/pub/Kain1998-ICASSP-Voice%20Conversion.pdf">Spectral Voice Conversion for Text-to-Speech Synthesis</a>&rdquo;, Proceedings of ICASSP, May 1998, vol.&nbsp;1, pp.&nbsp;285–288.</li>
<li class="enumerate_resume_item">S.&nbsp;Sutton, R.&nbsp;Cole, J.&nbsp;de Villiers, J.&nbsp;Schalkwyk, P.&nbsp;Vermeulen, M.&nbsp;Macon, Y.&nbsp;Yan, E.&nbsp;Kaiser, B.&nbsp;Rundle, K.&nbsp;Shobaki, P.&nbsp;Hosom, A.<b>&nbsp;Kain</b>, J.&nbsp;Wouters, D.&nbsp;Massaro, M.&nbsp;Cohen, &ldquo;<a href="http://cslu.ohsu.edu/~kain/pub/Sutton1998-ICSLP-CSLU%20Toolkit.pdf">Universal Speech Tools: The CSLU Toolkit</a>&rdquo;, Proceedings of ICSLP, November 1998, vol.&nbsp;7, pp.&nbsp;3221–24.</li>
<li class="enumerate_resume_item">N.&nbsp;Malayath, H.&nbsp;Hermansky, A.<b>&nbsp;Kain</b> and R.&nbsp;Carlson, &ldquo;<a href="http://cslu.ohsu.edu/~kain/pub/Malayath1997-Eurospeech-SI%20by%20PCA.pdf">Speaker-independent Feature Extraction by Oriented Principal Component Analysis</a>&rdquo;, Proceedings of EUROSPEECH 1997.</li>
</ol>
<h4 class="subsubsection_" id='magicparlabel-258'>Abstracts</h4>

<ol class="enumerate" id='magicparlabel-259'><li class="enumerate_item">F.&nbsp;van Brenk, A.&nbsp;Kain, and K.&nbsp;Tjaden, &ldquo;Investigating Intelligibility Gains for a Slowed Rate Using Hybridization&rdquo;, Twentieth Biennial Conference on Motor Speech, 2020.</li>
<li class="enumerate_item">D.&nbsp;Britton, A.&nbsp;<b>Kain</b>, Y-W.&nbsp;Chen, J.&nbsp;Wiedrick, J.&thinsp;O.&nbsp;Benditt, A.&thinsp;L.&nbsp;Merati, D.&nbsp;Graville, &ldquo;Extreme sawtooth sign in motor neuron disease (MND) suggests laryngeal resistance to forced expiratory airflow&rdquo;, Dysphagia Research Society 28th Annual Meeting, 2020.</li>
<li class="enumerate_item">N.&nbsp;Sathe, A.&nbsp;<b>Kain</b>, and L.&nbsp;Reiss, &ldquo;Fusion and Identification of Dichotic Consonants in Normal-Hearing and Hearing-Impaired Listeners&rdquo;, ARO Mid-Winter Meeting, 2019.</li>
<li class="enumerate_item">D.&nbsp;Britton, A.&nbsp;<b>Kain</b>, Y-W.&nbsp;Chen, J.&nbsp;Wiedrick, J.&thinsp;O.&nbsp;Benditt, A.&thinsp;L.&nbsp;Merati, D.&nbsp;Graville, &ldquo;Extreme sawtooth sign in motor neuron disease (MND) suggests laryngeal resistance to forced airflow&rdquo;, Fall Voice Conference, 2018.</li>
<li class="enumerate_item">B.&nbsp;<u>Snider</u> and A.&nbsp;<b>Kain</b>, &ldquo;Estimation of Localized Ideal Oximetry Sensor Lag via Oxygen Desaturation--Disordered Breathing Event Cross-Correlation&rdquo;, SLEEP: Journal of Sleep and Sleep Disorders Research, 40, page A232, 2017.</li>
<li class="enumerate_item">J.-P.&nbsp;Hosom, A.<b>&nbsp;Kain</b>, and B.<u>&nbsp;Bush</u>, &ldquo;<a href="http://scitation.aip.org/content/asa/journal/jasa/130/4/10.1121/1.3654650">Towards the recovery of targets from coarticulated speech for automatic speech recognition</a>&rdquo;, The Journal of the Acoustical Society of America, 130(4), page 2407, 2011.</li>
<li class="enumerate_item">A.<b>&nbsp;Kain</b>, "Speech transformation: Increasing intelligibility and changing speakers", Journal of the Acoustical Society of America, 126(4), page 2205, 2009.</li>
</ol>
<h4 class="subsubsection_" id='magicparlabel-266'>Ph.&thinsp;D.&nbsp;Thesis</h4>
<div class="standard" id='magicparlabel-267'>&ldquo;<a href="http://cslu.ohsu.edu/~kain/pub/Kain2001-Thesis-Voice%20Conversion.pdf">High Resolution Voice Transformation</a>&rdquo;, OGI School of Science &amp; Engineering, 2001.</div>
<h4 class="subsubsection_" id='magicparlabel-268'>Technical Reports</h4>

<ol class="enumerate" id='magicparlabel-269'><li class="enumerate_item">B.&thinsp;R.&nbsp;<u>Snider</u> and A.&nbsp;<b>Kain</b>, &ldquo;Adaptive Reduction of Additive Noise from Sleep Breathing Sounds&rdquo;, CSLU-2012-001.</li>
<li class="enumerate_item">A.&nbsp;<b>Kain</b>, J.-P.&nbsp;Hosom, S.&thinsp;H.&nbsp;Ferguson, B.<u>&nbsp;Bush</u>, &ldquo;Creating a speech corpus with semi-spontaneous, parallel conversational and clear speech&rdquo;, CSLU-11-003.</li>
</ol>
<h4 class="subsubsection_" id='magicparlabel-271'>Patents</h4>

<ol class="enumerate" id='magicparlabel-272'><li class="enumerate_item">J.&nbsp;van Santen and A.&nbsp;<b>Kain</b>, OHSU. System and Method for Compressing Concatenative Acoustic Inventories for Speech Synthesis.</li>
<li class="enumerate_item">A.&nbsp;<b>Kain</b> and Y.&nbsp;Stylianou, AT&amp;T Research Laboratories. Stochastic Modeling Of Spectral Adjustment For High Quality Pitch Modification.</li>
</ol>
<h4 class="subsubsection_" id='magicparlabel-274'>Datasets</h4>

<ul class="itemize" id='magicparlabel-275'><li class="itemize_item">A.&nbsp;<b>Kain</b>, The VOICES dataset, Linguistic Data Consortium Catalog, LDC2006S01, ISBN 1-58563-363-1, 2006.</li>
</ul>
<h4 class="subsubsection_" id='magicparlabel-276'>OHSU Disclosures</h4>

<ol class="enumerate" id='magicparlabel-277'><li class="enumerate_item">#2805 Decomposing autoencoder, 09/18/2019</li>
<li class="enumerate_item">#2803 Joint Deep autoencoder, 09/18/2019</li>
<li class="enumerate_item">#2717 Children's pronunciation database, 04/08/2019</li>
<li class="enumerate_item">#2489 TimeView software, 08/15/2017, <b>non-exclusively licensed under the MIT open-source license</b></li>
<li class="enumerate_item">#2275 PyTTS Text-to-Speech software with 16 voices, 05/12/2016, <b>Exclusively Licensed</b></li>
<li class="enumerate_item">#1365 Mexican Spanish female diphone voice, 12/08/2008</li>
<li class="enumerate_item">#1364 Mexican Spanish male diphone voice, 12/08/2008</li>
<li class="enumerate_item">#1362 American English female diphone voice (AS), 12/08/2008</li>
<li class="enumerate_item">#1361 American English male speaker diphone voice, 12/08/2008</li>
<li class="enumerate_item">#1360 German male speaker diphone voice, 12/08/2008</li>
<li class="enumerate_item">#1359 German female speaker diphone voice, 12/08/2008</li>
<li class="enumerate_item">#1358 New Flinger singing synthesis, 12/08/2008</li>
<li class="enumerate_item">#1195 Clear-Speech Corpus, Speaker JPH, 05/07/2007</li>
<li class="enumerate_item">#1065 Controlling Formant Frequencies in Concatenative Speech Synthesis Systems, 05/16/2006</li>
<li class="enumerate_item">#1061 Noninvasive Nasal Flow Measurement Device and Algorithm, 05/11/2006</li>
<li class="enumerate_item">#0868 CSLU System and Method for Synthesis Based Speech Enhancement, 09/24/2004, <b>Exclusively Licensed</b></li>
<li class="enumerate_item">#0844 CSLU Voice transformation for Dysarthria with Formant Re-synthesis, 06/03/2004, <b>Exclusively Licensed</b></li>
<li class="enumerate_item">#0665 Voice Transformation (High Resolution), 11/13/2002</li>
<li class="enumerate_item">#0566 Method to compress concatenative acoustic inventories for speech synthesis, 07/01/2001, <b>Exclusively Licensed</b>.</li>
</ol>
<h3 class="subsection" id='magicparlabel-296'><span class="subsection_label">4.5</span> Invited Lectures, Conference Presentations, or Professorships</h3>
<h4 class="subsubsection_" id='magicparlabel-297'>International and National</h4>

<ol class="enumerate" id='magicparlabel-298'><li class="enumerate_item">Conference presentation: &ldquo;Siamese Autoencoders for Speech Style Extraction and Switching Applied to Voice Identification and Conversion”, Interspeech, Stockholm, Sweden, 2017.</li>
<li class="enumerate_item">Conference presentation: &ldquo;A Comparison of Sentence-level Speech Intelligibility Metrics”, Interspeech, Stockholm, Sweden, 2017.</li>
<li class="enumerate_item">Conference presentation: &ldquo;Semi-supervised Training of a Voice Conversion Mapping Function using a Joint-Autoencoder&rdquo;, Interspeech, Dresden, Germany, 2016.</li>
<li class="enumerate_item">Conference presentation: &ldquo;Hybridizing Conversational and Clear Speech to Investigate the Source of Intelligibility Variation in Parkinson’s Disease&rdquo;, Conference on Motor Speech, Sarasota, Florida, 2014.</li>
<li class="enumerate_item">Conference presentation: &ldquo;Transmutative Voice Conversion&rdquo;, ICASSP, Vancouver, Canada, 2013.</li>
<li class="enumerate_item">Conference presentation: &rdquo;Frequency-domain delexicalization using surrogate vowels&rdquo;, Interspeech, Makuhari, Japan, 2010.</li>
<li class="enumerate_item">Conference presentation: &rdquo;Compression of Line Spectral Frequency Parameters using the Asynchronous Interpolation Model&rdquo;, 7th ISCA Workshop on Speech Synthesis, Kyoto, Japan, 2010.</li>
<li class="enumerate_item"><b>Invited</b> conference presentation: &rdquo;Hybridizing Conversational and Clear Speech to Determine the Degree of Contribution of Acoustic Features to Intelligibility&rdquo;, Meeting of the Acoustical Society of America, 2009, San Diego, CA.</li>
<li class="enumerate_item"><b>Invited</b> conference presentation for a Special Session on Voice Transformation: &rdquo;Using Speech Transformation to Increase Speech Intelligibility for the Hearing- and Speaking-impaired&rdquo;, ICASSP, Taipei, Taiwan, 2009.</li>
<li class="enumerate_item">Conference presentation: &rdquo;Compression of Line Spectral Frequency Parameters with Asynchronous Interpolation&rdquo;, ICASSP, Taipei, Taiwan, 2009.</li>
<li class="enumerate_item">Conference presentation: &rdquo;Hybridizing Conversational and Clear Speech&rdquo;, Interspeech, Antwerp, Belgium, 2007.</li>
<li class="enumerate_item">Conference presentation: &rdquo;Spectral Control in Concatenative Speech Synthesis&rdquo;, 6th ISCA Workshop on Speech Synthesis, Bonn, Germany, 2007.</li>
<li class="enumerate_item">Conference presentation: &rdquo;Unit-Selection Text-to-Speech Synthesis Using an Asynchronous Interpolation Model&rdquo;, 6th ISCA Workshop on Speech Synthesis, Bonn, Germany, 2007.</li>
<li class="enumerate_item">Conference presentation: &rdquo;Formant Re-synthesis of Dysarthric Speech&rdquo;, 5th ISCA Workshop on Speech Synthesis, Pittsburgh, PA, USA, 2004.</li>
<li class="enumerate_item">Conference presentation: &rdquo;A speech model of acoustic inventories based on asynchronous interpolation&rdquo;, EUROSPEECH, Geneva, Switzerland, 2003.</li>
<li class="enumerate_item">Conference presentation: &rdquo;Compression of Acoustic Inventories using Asynchronous Interpolation&rdquo;, IEEE Workshop on Speech Synthesis, Santa Monica, CA, 2002.</li>
<li class="enumerate_item">Conference presentation: &rdquo;Design and Evaluation of a Voice Conversion Algorithm based on Spectral Envelope Mapping and Residual Prediction&rdquo;, ICASSP, Salt Lake City, UT, 2001.</li>
<li class="enumerate_item">Conference presentation: &rdquo;Spectral Voice Conversion for Text-to-Speech Synthesis&rdquo;, ICASSP, Seattle, WA, 1998.</li>
</ol>
<h4 class="subsubsection_" id='magicparlabel-316'>Regional and Local</h4>

<ul class="itemize" id='magicparlabel-317'><li class="itemize_item">Presentation at CSLU Seminar Series approximately 1–2 times annually</li>
</ul>
<h3 class="subsection" id='magicparlabel-318'><span class="subsection_label">4.6</span> Awards</h3>

<ul class="itemize" id='magicparlabel-319'><li class="itemize_item">2017, 2013 OHSU Technology Transfer and Business Development Award</li>
<li class="itemize_item">2017 NVIDIA Academic Hardware Donation Program</li>
<li class="itemize_item">2005 OHSU Commercialization Award</li>
</ul>
<h2 class="section" id='magicparlabel-322'><span class="section_label">5</span> Service</h2>
<h3 class="subsection" id='magicparlabel-323'><span class="subsection_label">5.1</span> Membership in Professional Societies</h3>

<ul class="itemize" id='magicparlabel-324'><li class="itemize_item">International Speech Communication Association (ISCA)</li>
<li class="itemize_item">Institute of Electrical and Electronics Engineers (IEEE)</li>
<li class="itemize_item">Acoustical Society of America (ASA)</li>
</ul>
<h3 class="subsection" id='magicparlabel-327'><span class="subsection_label">5.2</span> Granting Agency Review Work</h3>

<ul class="itemize" id='magicparlabel-328'><li class="itemize_item">I served as a grant reviewer for the Spinal Cord Injury/Disease Research Program, 2018.</li>
<li class="itemize_item">I served as an additional reviewer on one grant proposal for the National Science Foundation, 2013.</li>
<li class="itemize_item">National Science Foundation, April, 2010. I reviewed several grant proposals, and then participated in two-day panel discussions.</li>
</ul>
<h3 class="subsection" id='magicparlabel-331'><span class="subsection_label">5.3</span> Editorial and Ad Hoc Review Activities</h3>

<ul class="itemize" id='magicparlabel-332'><li class="itemize_item">I typically review 1–3 <b>journal articles</b> annually. I have reviewed for:

<ul class="itemize" id='magicparlabel-333'><li class="itemize_item">Journal of the Acoustical Society of America</li>
<li class="itemize_item">Journal of Computer, Speech, and Language</li>
<li class="itemize_item">IEEE Transactions on Audio, Speech and Language Processing</li>
<li class="itemize_item">Speech Communication Journal</li>
<li class="itemize_item">Transactions on Accessible Computing</li>
<li class="itemize_item">Transactions on Asian and Low-Resource Language Information Processing</li>
<li class="itemize_item">Journal of Speech, Language, and Hearing Research</li>
<li class="itemize_item">International Journal of Speech-Language Pathology</li>
</ul>
</li><li class="itemize_item">I review a combined 4–8 <b>conference papers</b> annually for international conferences Interspeech and ICASSP. These conference papers are five pages long, and are scored along several dimensions.</li>
<li class="itemize_item"><b>Guest Editor</b> for a special Voice Transformation issue of the IEEE Transactions on Audio, Speech and Language Processing Journal, Volume 18, Issue 5, July 2010. My responsibilities included co-organization of the issue publication, co-authoring the introduction, and reviewing articles.</li>
</ul>
<h3 class="subsection" id='magicparlabel-343'><span class="subsection_label">5.4</span> Committees</h3>
<h4 class="subsubsection_" id='magicparlabel-344'>International/National</h4>

<ul class="itemize" id='magicparlabel-345'><li class="itemize_item"><b>Publications Chair</b> for the international conference InterSpeech 2012 in Portland Oregon. Over several months, I coordinated with the Technical Program Committee, the Organizing Committee, and the Professional Conference Organizer to produce the electronic proceedings and the abstract book.</li>
</ul>
<h4 class="subsubsection_" id='magicparlabel-346'>Departmental</h4>

<ul class="itemize" id='magicparlabel-347'><li class="itemize_item">I participate in multiple distinct <b>Dissertation Advisory Committee</b> (DAC, for Ph.&thinsp;D.&nbsp;students) and <b>Thesis Advisory Committee</b> (TAC, for Masters Students). These are typically semi-annual half-hour meetings wherein a student meets with his/her research advisor and other faculty to discuss progress, course work, and future plans.</li>
<li class="itemize_item">I participate in the annual <b>Qualifying Exam Committee</b>, an annual one-day meeting wherein pre-qualifying Ph.&thinsp;D.&nbsp;students present their Qualifying Exam Work to faculty and other students. Faculty are assigned to be readers on several papers, and the written work and presentation are scored along several dimensions.</li>
<li class="itemize_item">I participate in <b>Ph.&thinsp;D.&nbsp;Thesis Committees</b> as needed. Prior to the Ph.&thinsp;D.&nbsp;defense, several faculty are assigned as evaluators of the written thesis, a task that typically takes 1–3 days due to the volume of information (usually over 100 pages).</li>
<li class="itemize_item">Member of the <b>Admissions Committee</b> reviewing applications of M.&thinsp;S.&nbsp;and Ph.&thinsp;D.&nbsp;students to the CS/EE program.</li>
<li class="itemize_item">From 2014–2017 I was a member of the <b>Faculty Council Committee</b>, which makes available to the Dean informed representative faculty and departmental opinion, counsel, affairs, and problems of the Medical School, especially in areas of administrative and operational policies directly concerned with educational matters.</li>
<li class="itemize_item">I give <b>outreach talks</b> to introduce our center and its educational mission to other academic institutions. Most recent talks were at Reed University, April 2015; Intel, June 2015; George Fox University, December 2015; OHSU's summer talk series, July 2016.</li>
</ul>
<h3 class="subsection" id='magicparlabel-353'><span class="subsection_label">5.5</span> Activities</h3>

<ul class="itemize" id='magicparlabel-358'><li class="itemize_item">I created and maintain several open-source projects:

<ul class="itemize" id='magicparlabel-359'><li class="itemize_item"><b>TimeView</b> (<a href="https://github.com/lxkain/timeview">https://github.com/lxkain/timeview</a>) is a cross-platform (Windows, MacOS, Linux) desktop application for viewing and editing Waveforms, Time-Value data, and Segmentation data. These data can easily be analyzed or manipulated using a library of built-in processors; for example, a linear filter can operate on a waveform, or an activity detector can create a segmentation from a waveform. Processors can be easily customized or created from scratch.</li>
<li class="itemize_item"><b>Multiple Isotonic Regression</b> (<a href="https://github.com/lxkain/multi-isoreg">https://github.com/lxkain/multi-isoreg</a>) is an algorithm that, given a sequence, can find the minimum error and <em>any number</em> of optimal inflection points of segments that are either monotonically rising or falling. This allows finding shapes like up-down (one peak), or down-up-down, or up-down-up-down (2 peaks), etc. Special emphasis was placed on performance.</li>
<li class="itemize_item"><b>Joint-Density Regression</b> (<a href="https://github.com/lxkain/jd-reg">https://github.com/lxkain/jd-reg</a>) can create non-linear mapping functions using K-Means or GMMs.</li>
</ul>
</li><li class="itemize_item">I worked on designing a formal <b>mentee feedback mechanism</b> to mentors, as an outcome of the 2017 &ldquo;Making a Meaningful Difference&rdquo; Leadership class offered by OHSU's Niki Steckler.<div class="note_comment"><div class="plain_layout" id='magicparlabel-366'>I created and maintain CSLU's <b>unified educational computation environment</b>. This is implemented via a custom-installation of the latest Ubuntu Linux distribution, with the Python anaconda distribution pre-installed. The virtual disk image is distributed online and can be used with the free VirtualBox Virtual Machine application on Windows, OSX, and Linux alike.</div>
</div></li>
<li class="itemize_item">I manage and maintain CSLU's <b>Virtual Reality Laboratory</b>, which features a HTC Vive headset, driven by Valve's SteamVR software. Custom worlds can be created using Epic's Unreal Engine.</li>
<li class="itemize_item">I oversee operations and maintenance of CSLU's <b>Audio Laboratory</b> hardware and software systems, which feature two WhisperRoom sound-proof booths, a 10-channel 32-bit 96 kHz Focusrite recording audio interface connected to a Digital Audio Workstation running custom software, a Kay Elemetrics laryngograph for capturing a high-quality voicing signal, high-quality condenser microphones, numerous digital video cameras, a teleprompter, and other support equipment.<div class="note_comment"><div class="plain_layout" id='magicparlabel-372'>I designed the 2016 CSLU <b>printed educational catalog</b>.</div>
</div></li>
<li class="itemize_item">In 2012–2013, Izhak Shafran and I worked with <b>Portland State University</b> (PSU) on increasing educational collaboration. This required several meetings with key faculty and administrators, both at OHSU and PSU. As a result, CSLU and PSU cross listed certain Computer Science and Electrical Engineering courses. In 2016, this became an institution-wide agreement.</li>
</ul>
<h2 class="section" id='magicparlabel-374'><span class="section_label">6</span> Education</h2>
<h3 class="subsection" id='magicparlabel-375'><span class="subsection_label">6.1</span> Students</h3>

<ul class="itemize" id='magicparlabel-376'><li class="itemize_item">I have <b>mentored</b> <b>Ph.&thinsp;D.</b>&nbsp;students: Dinh, Snider, Mohammadi, Bush, Dudy, Khan, Moldover. I typically meet each of them one-on-one for 1–1.5 hours weekly to discuss their research. I have also <b>co-mentored</b> <b>Ph.&thinsp;D</b>.&nbsp;students: Langarani, Wallis, Resalat, Sathe, Bayestehtashk, Niu, Amano-Kusumoto, Miao. These students have found employment at Apple, Microsoft, Amazon, and other similar high-tech firms.</li>
<li class="itemize_item">I have <b>co-/mentored Master's</b> students: Roten, Soethiha, Yaeger, Alder, Velata, Moore.</li>
<li class="itemize_item">When appropriate, I lead weekly 1.5-hour <b>project group meetings</b> wherein 2–7 students and possibly additional faculty members report on and discuss their research with each other.</li>
<li class="itemize_item">When appropriate, I lead weekly 1-hour <b>reading group meetings</b>, wherein one student presents a published paper of his/her choice to a group of students and faculty (invitation is open to all of CSLU), with subsequent discussion.</li>
<li class="itemize_item">I supervised a total of 6 <b>undergraduate students</b> in the summers of 2008, 2009, and 2013, and 2016, funded through the National Science Foundation's Research Experiences for Undergraduates (REU) program and through the University Center for Excellence in Developmental Disabilities (UCEDD).</li>
<li class="itemize_item">I work with <b>volunteers</b> who would like to become Research Assistants, students, or co-authors on a publication with me.</li>
</ul>
<h3 class="subsection" id='magicparlabel-410'><span class="subsection_label">6.2</span> Courses</h3>
<div class="standard" id='magicparlabel-411'>Nearly all of my lectures are taught using <em>jupyter notebooks</em>, which, in addition to regular text and equations, allow for interactive code examples, graphical widgets, and data visualization during class. Students can download these notebooks and use them as reference or starting points for their own projects.</div>
<h4 class="subsubsection_" id='magicparlabel-412'>CS&nbsp;627 Data Science Programming</h4>
<div class="standard" id='magicparlabel-413'>This course is a <em>best-of</em> compilation of concepts, practices, and python-based software libraries (all free, open-source, and unrestricted) that allow for rapid, straight-forward, and easy-to-maintain implementation of new ideas and scientific questions. Students will gain awareness and initial working knowledge of some of the most fundamental computational tools for performing a wide variety of academic research. As such, it will focus on providing breadth instead of depth, which means that for each concept we will talk about motivation, key concepts, and concrete usage scenarios, but without exhaustive mathematical background or proofs, which can be acquired in more specialized classes. In this class we will: write programs in <em>python</em>; perform numeric tasks using <em>numpy</em> and <em>scipy</em>; manage data using <em>pandas</em>; discuss audio, image and text processing using <em>scipy.signal</em>, <em>scikit-image</em>, <em>nltk</em>, and <em>pynini</em>; apply machine learning algorithms such as deep neural networks, convolutional neural networks, and autoencoders using <em>scikit-learn</em> and <em>keras</em>; visualize data using <em>matplotlib</em> and <em>pyqtgraph</em>; use <em>pyqt/QT</em> to build graphical user interfaces; address performance issues via compilation/profiling/parallelization tools, and much more (winner of the 2019 Sakai Torchbearer Award).</div>

<div class="standard" id='magicparlabel-414'>I have created the curriculum for, and teach this 3-credit course (20<math xmlns="http://www.w3.org/1998/Math/MathML">
 <mrow><mo> &times; </mo>
 </mrow></math>1.5-hour lectures). Creating the curriculum required approximately 200 hours. Due to the quickly changing landscape at the edge of technology, updating and teaching the course requires approximately 100 hours each time it is offered. Grading students' answers and evaluating their project outcomes requires a total of approximately 3 hours per student over the course of the class (unless a TA is available). Students' evaluation scores averaged 5.0/5.0 in Fall 2015, 5.15/6.0 in Fall 2016, 5.64/6.0 in Winter 2018, 5.83/6.0 in Spring 2019.<div class="foot"><span class="foot_label">1</span><div class="foot_inner"><div class="plain_layout" id='magicparlabel-418'>As a point of reference, over the last 56 classes, the mean evaluation score was 4.3, and the median score was 4.4. Please refer to the OHSU Educators Portfolio document for further details.</div>
</div></div></div>
<h4 class="subsubsection_" id='magicparlabel-419'>EE&nbsp;682 Digital Signal Processing</h4>
<div class="standard" id='magicparlabel-420'>This course teaches students the core principals of digital signal processing. We survey a variety of topics in class lecture/discussion based on assigned readings while exploring specific topics/applications in depth through lab assignments and a final project. Specifically, we cover the core topic areas in digital signal processing including an overview of discrete-time signals and systems, the discrete-time Fourier transform, the <math xmlns="http://www.w3.org/1998/Math/MathML">
 <mrow><mi>z</mi>
 </mrow></math>-Transform and transform analysis, the discrete Fourier Series, the discrete Fourier transform, circular convolution, network structures for FIR systems, design of IIR and FIR filters, and multi-rate processing.</div>

<div class="standard" id='magicparlabel-421'>I co-teach this 3-credit course (10<math xmlns="http://www.w3.org/1998/Math/MathML">
 <mrow><mo> &times; </mo>
 </mrow></math>1.5-hour lectures). Creating the lectures required approximately 180 hours. Students' evaluation scores averaged 5.13/6.0 in Winter 2019.</div>
<h4 class="subsubsection_" id='magicparlabel-422'>EE&nbsp;658 Speech Signal Processing</h4>
<div class="standard" id='magicparlabel-423'>Speech systems are becoming commonplace in today's computer systems and Augmentative and Alternative Communication (AAC) devices. Topics include speech production and perception by humans, linear predictive features, pitch estimation, speech coding, speech enhancement, prosodic speech modification, Voice Conversion (VC), Text-to-Speech (TTS), and automatic speech recognition (ASR).</div>

<div class="standard" id='magicparlabel-424'>I have created the curriculum for, and teach this 3-credit course (20<math xmlns="http://www.w3.org/1998/Math/MathML">
 <mrow><mo> &times; </mo>
 </mrow></math>1.5-hour lectures). Creating the curriculum required approximately 180 hours. Due to the quickly changing landscape at the edge of technology, updating and teaching the course requires approximately 200 hours each time it is offered. Grading students' answers and evaluating their project outcomes requires a total of approximately 3 hours per student over the course of the class (unless a TA is available). Students' evaluation scores averaged 4.7/5.0 in Winter 2016.</div>
<h4 class="subsubsection_" id='magicparlabel-425'>CS&nbsp;653 Text-to-Speech Synthesis</h4>
<div class="standard" id='magicparlabel-426'>This course will introduce students to the problem of synthesizing speech from text input. Speech synthesis is a challenging area that draws on expertise from a diverse set of scientific fields, including signal processing, linguistics, psychology, statistics, and artificial intelligence. Fundamental advances in each of these areas will be needed to achieve truly human-like synthesis quality and advances in other realms of speech technology (like speech recognition, speech coding, speech enhancement). In this course, we will consider current approaches to sub-problems such as text analysis, pronunciation, linguistic analysis of prosody, and generation of the speech waveform. Lectures, demonstrations, and readings of relevant literature in the area will be supplemented by student lab exercises using hands-on tools.</div>

<div class="standard" id='magicparlabel-427'>I have created the curriculum for and teach the second half of this 3-credit course (10<math xmlns="http://www.w3.org/1998/Math/MathML">
 <mrow><mo> &times; </mo>
 </mrow></math>1.5-hour lectures). Creating the curriculum required approximately 120 hours of my time. Students' evaluation scores averaged 4.4/5.0 in 2015.</div>
<h4 class="subsubsection_" id='magicparlabel-428'>CS&nbsp;606 Computational Approaches to Speech and Language Disorders</h4>
<div class="standard" id='magicparlabel-429'>This course covers a range of speech and language analysis algorithms that have been developed for measurement of speech or language based markers of neurological disorders, for the creation of assistive devices, and for remedial applications. Topics include introduction to speech and language disorders, robust speech signal processing, statistical approaches to pitch and timing modeling, voice transformation algorithms, speech segmentation, and modeling of disfluency. The class uses a wide array of clinical data, and is closely tied to several ongoing research projects.</div>

<div class="standard" id='magicparlabel-430'>I have created and taught 2<math xmlns="http://www.w3.org/1998/Math/MathML">
 <mrow><mo> &times; </mo>
 </mrow></math>1.5-hour lectures for this course.</div>
<h3 class="subsection" id='magicparlabel-431'><span class="subsection_label">6.3</span> Presentations</h3>

<ul class="itemize" id='magicparlabel-432'><li class="itemize_item"><b>Speaker</b> at the 2017 OHSU Symposium on Educational Excellence, on &ldquo;Interactive lecturing with jupyter notebooks&rdquo;.</li>
<li class="itemize_item">Twice yearly <b>Course Advertisement Talks</b> to preview my courses that are scheduled the following quarter to prospective students.</li>
</ul>
<h3 class="subsection" id='magicparlabel-434'><span class="subsection_label">6.4</span> Awards</h3>

<ul class="itemize" id='magicparlabel-435'><li class="itemize_item">2019 Sakai Torchbearer Award for the use of jupyter notebooks in education</li>
<li class="itemize_item">2017 nominated for OHSU Excellence in Graduate Teaching Award</li>
<li class="itemize_item">2014 OHSU Excellence in Graduate Teaching Award</li>
</ul>
</body>
</html>
