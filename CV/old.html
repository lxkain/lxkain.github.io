<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN">
<html>
<head>
	<meta http-equiv="content-type" content="text/html; charset=utf-8"/>
	<title>Alexander Kain at CSLU/OHSU</title>
	<meta name="generator" content="LibreOffice 5.2.3.3 (MacOSX)"/>
	<meta name="created" content="00:00:00"/>
	<meta name="changed" content="2017-01-24T12:26:58.617215000"/>
	<meta name="created" content="00:00:00">
	<meta name="changed" content="2016-07-13T17:08:33.060211000">
	<meta name="created" content="00:00:00">
	<meta name="changed" content="2016-01-16T17:17:34.057675000">
	<meta name="created" content="00:00:00">
	<meta name="changed" content="2016-01-16T17:11:46.279231000">
	<meta name="created" content="00:00:00">
	<meta name="changed" content="2016-01-16T17:09:40.449356000">
	<meta name="created" content="00:00:00">
	<meta name="changed" content="2016-01-16T17:06:59.796755000">
	<style type="text/css">
		h2.cjk { font-family: "SimSun" }
		h2.ctl { font-family: "Lucida Sans" }
		h3.cjk { font-family: "SimSun" }
		h3.ctl { font-family: "Lucida Sans" }
	</style>
</head>
<body lang="en-US" dir="ltr">
<table width="872" cellpadding="0" cellspacing="0" style="page-break-before: always">
	<col width="184">
	<col width="688">
	<tr>
		<td width="184" style="border: none; padding: 0in">
			<p><img src="self.jpg" name="Image1" align="left" width="181" height="273" border="0"/>
<br/>

			</p>
		</td>
		<td width="688" style="border: none; padding: 0in">
			<h1>Alexander Kain (<a href="mailto:kaina@ohsu.edu">kaina@ohsu.edu</a>)</h1>
			<h2 class="western">Computer Science &amp; Electrical Engineering
			(<a href="http://www.ohsu.edu/csee">CSEE</a>) <br/>
Center for
			Spoken Language Understanding (<a href="http://www.ohsu.edu/cslu">CSLU</a>)<br/>
Institute
			on Development &amp; Disability (<a href="http://www.ohsu.edu/xd/research/centers-institutes/institute-on-development-and-disability/index.cfm/reknew/">IDD</a>)<br/>
School
			of Medicine (<a href="http://www.ohsu.edu/xd/education/schools/school-of-medicine">SOM</a>)<br/>
Oregon
			Health &amp; Science University (<a href="http://www.ohsu.edu/">OHSU</a>)</h2>
			<p>ORCID <a href="https://orcid.org/0000-0001-5807-9311">0000-0001-5807-9311</a></p>
		</td>
	</tr>
</table>
<h2 class="western">Positions</h2>
<ul>
	<li/>
<p style="margin-bottom: 0in"><a href="http://www.ohsu.edu/">Oregon
	Health &amp; Science University</a>, Portland, OR<br/>
Associate
	Professor, 2014-present<br/>
Assistant Professor, 2007-2014<br/>
Senior
	Research Associate, 2005-2007</p>
	<li/>
<p style="margin-bottom: 0in"><a href="http://www.biospeech.com/">BioSpeech,
	Inc.</a>, Portland, OR<br/>
Chief Scientist, 2005-present</p>
	<li/>
<p style="margin-bottom: 0in"><a href="http://www.sensoryinc.com/">Sensory,
	Inc.</a>, Santa Clara, CA<br/>
Lead Speech Synthesis Technologist,
	2001-2008</p>
	<li/>
<p style="margin-bottom: 0in"><a href="http://www.research.att.com/">AT&amp;T
	Research Labs</a>, Florham Park, NJ<br/>
Visiting Researcher, 1999</p>
	<li/>
<p>Reviewer / Guest Editor for: Journal of the Acoustical
	Society of America (JASA); Computer, Speech, and Language; Journal
	of Speech, Language, and Hearing Research (JSLHR); IEEE Journals;
	scientific conferences such as Interspeech; National Science
	Foundation (NSF) proposals.</p>
</ul>
<h2 class="western">Education</h2>
<ul>
	<li/>
<p style="margin-bottom: 0in"><a href="http://www.ogi.edu/">OGI
	School of Science &amp; Engineering</a>, Portland, OR<br/>
Postdoctoral
	Training, 2002-2005</p>
	<li/>
<p style="margin-bottom: 0in"><a href="http://www.ogi.edu/">Oregon
	Graduate Institute</a>, Portland, OR<br/>
Ph.D. in Computer Science
	and Engineering, 2001</p>
	<li/>
<p><a href="http://www.rockford.edu/">Rockford College</a>,
	Rockford, IL<br/>
Double B.A. in Computer Science &amp; Mathematics,
	1995</p>
</ul>
<h2 class="western">Research Support</h2>
<h3 class="western">Current</h3>
<ul>
	<li/>
<p style="margin-bottom: 0in">2015/09/01-2020/08/31 National
	Institutes of Health 2R01DC004689-11A1, &quot;Therapeutic Approaches
	to Dysarthria: Acoustic and Perceptual Correlates&quot;, PI: Tjaden
	(U at Buffalo). To address incomplete knowledge of the comparative
	merits of dysarthria therapy techniques by comparing the acoustic
	and perceptual consequences of rate reduction, increased vocal
	intensity and clear speech variants in MS and PD.</p>
	<li/>
<p style="margin-bottom: 0in">2014/09/01-2017/08/31: National
	Institutes of Health 1R43MH101978-01A1, &quot;System for automatic
	classification of rodent vocalizations&quot;, PI: Lahvis
	(BioSpeech). Development of treatments for neuropsychiatric
	disorders presents a formidable challenge. To advance drug
	discovery, assessments of laboratory rodents are widely employed by
	academia and industry to model neuropsychiatric disorders.
	Substantial recent advances in digital recordings of rodent
	ultrasonic vocalizations (USVs) have engendered interest in
	assessment of USVs to measure behavior change. A practical obstacle
	to USV assessment is that they are classified manually. We propose a
	software system that allows a user to rapidly interrogate recordings
	of rodent USVs for prosodic content.</p>
	<li/>
<p>2010/09/27-2016/09/30: National Science Foundation
	BCS-1027834, &quot;<a href="http://www.nsf.gov/awardsearch/showAward?AWD_ID=1027834">Computational
	Models for the Automatic Recognition of Non-Human Primate Social
	Behaviors</a>&quot;, PI: Kain (OHSU). To develop methods that will
	permit researchers to remotely and automatically monitor behavior of
	primates and other highly social animals.</p>
</ul>
<h3 class="western">Completed</h3>
<ul>
	<li/>
<p style="margin-bottom: 0in">2013/09/15-2016/02/28: National
	Institutes of Health 1R43DA037588-01A1, &quot;<a href="http://projectreporter.nih.gov/project_info_details.cfm?aid=8648375&amp;icde=17801918">Screening
	for Sleep Disordered Breathing with Minimally Obtrusive Sensors</a>&quot;,
	PI: Snider (BioSpeech). To create a portable, low-cost, and
	minimally obtrusive system for automatically detecting
	sleep-disordered breathing, such as cessation of breathing (apnea).</p>
	<li/>
<p style="margin-bottom: 0in">2011/12/01-2015/08/31: National
	Institute of Health R21DC012139, &quot;<a href="http://projectreporter.nih.gov/project_info_description.cfm?aid=8336853&amp;icde=17803266">Computer-Based
	Pronunciation Analysis for Children with Speech Sound Disorders</a>&quot;,
	PI: Kain (OHSU). The aim is to develop speech-production assessment
	and pronunciation training tools for children with speech sound
	disorders.</p>
	<li/>
<p style="margin-bottom: 0in">2010/05/15-2015/04/30: National
	Science Foundation IIS-0964468, &quot;<a href="http://www.nsf.gov/awardsearch/showAward.do?AwardNumber=0964468">HCC:
	Medium: Synthesis and Perception of Speaker Identity&quot;</a>, PI:
	Kain (OHSU). To achieve the goal of synthesis of speaker identity
	from a small training corpus the project will address problems
	including trainable abstract parameterizations of the prosodic
	patterns that characterize a speaker and voice conversion methods.</p>
	<li/>
<p style="margin-bottom: 0in">2012/04/01-2015/03/31: National
	Institute of Health 5R44DC009515-03, &quot;<a href="http://projectreporter.nih.gov/project_info_details.cfm?aid=8249393&amp;icde=17812441">SBIR
	Phase II: Computer-based auditory skill building program for aural
	(re)habilitation</a>&quot;, PI: Connors (BioSpeech). To extend an
	adaptive computer-guided software program (the Speech Identification
	Tutor, SIT) that focuses on learning phoneme discrimination and
	identification.</p>
	<li/>
<p style="margin-bottom: 0in">2010/06/09-2014/08/31: National
	Science Foundation IIS-0964102, &quot;<a href="http://nsf.gov/awardsearch/showAward?AWD_ID=0964102">Semi-Supervised
	Discriminative Training of Language Models</a>&quot;, PI: Kain
	(OHSU). To conduct fundamental research in statistical language
	modeling to improve human language technologies, including automatic
	speech recognition (ASR) and machine translation (MT).&nbsp;</p>
	<li/>
<p style="margin-bottom: 0in">2011/04/01-2012/03/31: National
	Institute of Health 5R42DC008712, &quot;<a href="http://projectreporter.nih.gov/project_info_details.cfm?aid=8061617&amp;icde=10206352">User
	Adaptation of AAC Device Voices - Phase 2</a>&quot;, PI: Klabbers
	(BioSpeech). Developing and evaluating voice transformation and
	prosody modification technologies to customize synthetic voices in
	AAC devices, mimicking the individual user's pre-morbid speech.</p>
	<li/>
<p style="margin-bottom: 0in">2011/03/01-2013/03/31: National
	Institute of Health 1R43DC011706-01, &quot;<a href="http://www.sbir.gov/sbirsearch/detail/368241">SBIR
	Phase I: Computerized System for Phonemic Awareness Intervention</a>&quot;,
	PI: Connors (BioSpeech). This grant aims to develop and evaluate a
	play-and-drag-and-drop audio-visual interface for analyzing and
	sequencing phonemes in words to help children build the phonemic and
	phonological awareness foundational skills necessary for literacy.</p>
	<li/>
<p style="margin-bottom: 0in">2009/09/01-2013/08/31: National
	Science Foundation IIS-0915754, &quot;<a href="http://www.nsf.gov/awardsearch/showAward.do?AwardNumber=0915754">RI:
	Small: Modeling Coarticulation for Automatic Speech Recognition&quot;</a>,
	PI: Kain (OHSU). Performing automatic speech recognition (ASR) using
	the Asynchronous Interpolation Model (AIM) framework. By decomposing
	the input speech signal into basis vectors and weights, we search
	for phonemic basis vectors and weights that yield the
	highest-probability match to the input signal.</p>
	<li/>
<p style="margin-bottom: 0in">2009/07/15-2012/06/30: National
	Science Foundation IIS-0905095, &quot;<a href="http://www.nsf.gov/awardsearch/showAward.do?AwardNumber=0905095">HCC:
	Automatic detection of atypical patterns in cross-modal affect</a>&quot;,
	PI: van Santen (OHSU). The long term goal is to build interactive,
	agent based systems for (1) remediation of poor affect communication
	and (2) diagnosis of the underlying neurological disorders based on
	analysis of affective signals.</p>
	<li/>
<p style="margin-bottom: 0in">2009/07/17-2012/06/30: National
	Institute of Health 5R21DC010035, &quot;<a href="http://projectreporter.nih.gov/project_info_details.cfm?aid=7895680&amp;icde=10206236">Quantitative
	Modeling of Segmental Timing in Dysarthria</a>&quot;, PI: van Santen
	(OHSU). The project seeks to apply a quantitative modeling framework
	to segment durations in sentences produced by speakers with a
	variety of neurological diagnoses and dysarthrias.</p>
	<li/>
<p style="margin-bottom: 0in">2008-2009: Nancy Lurie Marks
	Family Foundation, &quot;<a href="http://nlmfoundation.org/grants/past_grants/communication_past.html">In
	Your Own Voice: Personal AAC Voices for Minimally Verbal Children
	with Autism Spectrum Disorder</a>&quot;, PI: van Santen (OHSU).
	Adapted a text-to-speech voice to sound like a child's voice.</p>
	<li/>
<p style="margin-bottom: 0in">2007/09/01-2011/08/31: National
	Science Foundation IIS-0713617, &quot;<a href="http://www.nsf.gov/awardsearch/showAward.do?AwardNumber=0713617">HCC:
	High-quality Compression, Enhancement, and Personalization of
	Text-to-Speech Voices</a>&quot;, PI: Kain (OHSU). Developed
	Text-to-Speech technologies that focus on elimination of
	concatenation errors, and accurate speech modifications in the areas
	of coarticulation, degree of articulation, prosodic effects, and
	speaker characteristics, using an asynchronous interpolation model.</p>
	<li/>
<p style="margin-bottom: 0in">2007/01/01-2008/06/30: National
	Institute of Health 1R41DC008712, &quot;<a href="http://projectreporter.nih.gov/project_info_details.cfm?aid=7219057&amp;icde=10206236">User
	Adaptation of AAC Device Voices - Phase 1</a>&quot;, PI: van Santen
	(BioSpeech). Developed and evaluated voice transformation and
	prosody modification technologies to customize synthetic voices in
	AAC devices, mimicking the individual user's pre-morbid speech.</p>
	<li/>
<p style="margin-bottom: 0in">2006/09/01-2008/03/31: National
	Institute of Health 1R41DC007240, &quot;<a href="http://projectreporter.nih.gov/project_info_details.cfm?aid=7162050&amp;icde=10206236">Voice
	Transformation for Dysarthria - Phase 1</a>&quot;, PI: van Santen
	(BioSpeech). Developed software that transforms speech compromised
	by dysarthria into easier-to understand and more natural-sounding
	speech. The software resides on a wearable computer, with headset
	microphone input and powered speaker or line output.</p>
	<li/>
<p style="margin-bottom: 0in">2005/01/10-2010/12/31: National
	Institute of Health 5R01DC007129, &quot;<a href="http://projectreporter.nih.gov/project_info_details.cfm?aid=7546551&amp;icde=10206330">Expressive
	crossmodal affect integration in Autism</a>&quot;, PI: van Santen
	(OHSU). This study performed a comprehensive analysis of crossmodal
	integration of affect expression in ASD.</p>
	<li/>
<p style="margin-bottom: 0in">2005/01/01-2006/06/30: National
	Science Foundation IIP-0441125, &quot;<a href="http://www.nsf.gov/awardsearch/showAward.do?AwardNumber=0441125">STTR
	Phase 1: Small Footprint Speech Synthesis</a>&quot;, PI: Kain
	(BioSpeech). Created and evaluated speech compression technologies
	for concatenative text-to-speech synthesizers.</p>
	<li/>
<p>2001/10/01-2005/09/30: National Science Foundation
	IIS-0117911, &quot;<a href="http://www.nsf.gov/awardsearch/showAward.do?AwardNumber=0117911">Making
	Dysarthric Speech Intelligible</a>&quot;, PI: van Santen (OHSU).
	Developed new algorithms that enable dysarthric individuals to be
	more easily understood by the general population.</p>
</ul>
<h2 class="western">Courses</h2>
<h3 class="western">CS 627 - Data Science Programming</h3>
<p>Credits: 3<br/>
This course represents a best-of compilation of
concepts, practices, and R- and python-based software libraries (all
free, open-source, and unrestricted) that allow for a relatively
rapid, straight-forward, and easy-to-maintain implementation of new
ideas and scientific questions. Students will gain awareness and
initial working knowledge of some of the most fundamental
computational tools for performing a wide variety of academic
research. As such, it will focus on providing breadth instead of
depth, which means that for each concept we will talk about
motivation, key concepts, and concrete usage scenarios, but without
mathematical background or proofs, which can be acquired in more
specialized classes. In this class we will: use R for data
exploration and visualization, write programs in python, perform
numeric tasks using numpy and scipy, analyze data using pandas,
discuss audio and image processing using scipy.signal and
scikit-image, apply machine learning algorithms using scikit-learn,
visualize data using matplotlib and pyqtgraph, use QT to build
graphical user interfaces, learn how to version control files with
git, address performance issues via
compilation/profiling/parallelization tools, and much more.</p>
<h3 class="western">EE 658 - Speech Signal Processing</h3>
<p>Credits: 3<br/>
Speech systems are becoming commonplace in today's
computer systems and Augmentative and Alternative Communication (AAC)
devices. Examples are speech recognition systems and Text-to-Speech
synthesis systems. This course will introduce the fundamentals of the
underlying speech signal processing that enables such systems. Topics
include speech production and perception by humans, frequency
transforms, filters, linear predictive features, pitch estimation,
speech coding, speech enhancement, and prosodic speech modification.</p>
<h3 class="western">CS 653 - Speech Synthesis</h3>
<p>Credits: 3<br/>
This course will introduce students to the problem
of synthesizing speech from text input. Speech synthesis is a
challenging area that draws on expertise from a diverse set of
scientific fields, including signal processing, linguistics,
psychology, statistics, and artificial intelligence. Fundamental
advances in each of these areas will be needed to achieve truly
human-like synthesis quality and advances in other realms of speech
technology (like speech recognition, speech coding, speech
enhancement). In this course, we will consider current approaches to
sub-problems such as text analysis, pronunciation, linguistic
analysis of prosody, and generation of the speech waveform. Lectures,
demonstrations, and readings of relevant literature in the area will
be supplemented by student lab exercises using hands-on tools.</p>
<h3 class="western">CS 606 - Computational Approaches to Speech and
Language Disorders</h3>
<p>Credits: 3<br/>
This course covers a range of speech and language
analysis algorithms that have been developed for measurement of
speech or language based markers of neurological disorders, for the
creation of assistive devices, and for remedial applications. Topics
will include introduction to speech and language disorders, robust
speech signal processing, statistical approaches to pitch and timing
modeling, voice transformation algorithms, speech segmentation, and
modeling of disfluency. The class will use a wide array of clinical
data, and will be closely tied to several ongoing research projects.</p>
<h2 class="western">Peer-reviewed Publications</h2>
<h3 class="western">Intelligibility</h3>
<p style="margin-bottom: 0.2in">Speech Intelligibility is the degree
to which listeners can understand a speech signal's message.
Historically, the specific acoustic sources of intelligibility are
poorly understood, and automatic approaches to modify the degree of
intelligibility were limited. We invented a hybridization approach
that allows for precisely measuring the degree of contribution of one
or more acoustic features to speech intelligibility. We applied this
approach to find the most relevant acoustic features that cause the
intelligibility improvement in clearly-spoken typical and dysarthric
speech. This allows a principled study of different remedial
strategies. We also created algorithms that automatically improve the
intelligibility of dysarthric or conversational speech signals, using
approaches from speech analysis, machine learning, and speech
synthesis. These algorithms may be instrumental for next-generation
hearing- and speaking-aids.</p>
<ul>
	<li/>
<p style="margin-bottom: 0in">K. Tjaden, A. Kain, J. Lam,
	&quot;<a href="http://jslhr.pubs.asha.org/article.aspx?articleID=1833542">Hybridizing
	Conversational and Clear Speech to Investigate the Source of
	Increased Intelligibility in Parkinson's Disease</a>&quot;, Journal
	of Speech, Language, and Hearing Research, Volume 57, August 2014.
	(<a href="https://www.youtube.com/watch?v=tnOJgMPxBZg">Video</a>)</p>
	<li/>
<p style="margin-bottom: 0in">S. Mohammadi, A. Kain, J. van
	Santen, &quot;<a href="pub/Mohammadi2012-Interspeech-Clear%20Vowels.pdf">Making
	Conversational Vowels More Clear</a>&quot;, Proceedings of
	Interspeech, 2012.</p>
	<li/>
<p style="margin-bottom: 0in">A. Kain and J. van Santen,
	&quot;<a href="pub/Kain2010-IS-Delex.pdf">Frequency-domain
	delexicalization using surrogate vowels</a>&quot;, Interspeech,
	2010.</p>
	<li/>
<p style="margin-bottom: 0in">A. Kain, J. van Santen, &quot;<a href="pub/Kain2009-ICASSP-Transformation.pdf">Using
	Speech Transformation to Increase Speech Intelligibility for the
	Hearing- and Speaking-impaired</a>&quot;, Proceedings of ICASSP,
	April 2009.</p>
	<li/>
<p style="margin-bottom: 0in">A. Kain, A. Amano-Kusumoto, and
	J.-P. Hosom, &quot;<a href="pub/Kain2008-JASA-Hybridizing.pdf">Hybridizing
	Conversational and Clear Speech to Determine the Degree of
	Contribution of Acoustic Features to Intelligibility</a>&quot;,
	Journal of the Acoustical Society of America, Volume 124, Issue 4,
	October 2008, pages 2308-2319.</p>
	<li/>
<p style="margin-bottom: 0in">A. Kusumoto, A. Kain, P. Hosom,
	and J. van Santen, &quot;<a href="pub/Kusumoto2007-Interspeech-Hybridizing.pdf">Hybridizing
	Conversational and Clear Speech</a>&quot;, Proceedings of
	Interspeech, August 2007.</p>
	<li/>
<p style="margin-bottom: 0in">A. Kain, J. Hosom, X. Niu, J.
	van Santen, M. Fried-Oken, J. Staehely, &quot;<a href="http://dx.doi.org/10.1016/j.specom.2007.05.001">Improving
	the Intelligibility of Dysarthric Speech</a>&quot;, Speech
	Communication, Volume 49, Issue 9, September 2007, Pages 743-759.</p>
	<li/>
<p style="margin-bottom: 0in">X. Niu, A. Kain, J. van Santen,
	&quot;<a href="pub/Niu2006-Interspeech-Velopharyngeal.pdf">A
	Noninvasive, Low-cost Device to Study the Velopharyngeal Port During
	Speech and Some Preliminary Results</a>&quot;, Proceedings of
	Interspeech, September 2006.</p>
	<li/>
<p style="margin-bottom: 0in">X. Niu, A. Kain, J. van Santen,
	&quot;<a href="pub/Niu2005-Eurospeech-Nasal.pdf">Estimation of the
	Acoustic Properties of the Nasal Tract during the Production of
	Nasalized Vowels</a>&quot;, Proceedings of EUROSPEECH, September
	2005.</p>
	<li/>
<p style="margin-bottom: 0in">A. Kain, X. Niu, J. Hosom, Q.
	Miao, J. van Santen, &quot;<a href="pub/Kain2004-SSW5-Formant%20Resynthesis.pdf">Formant
	Re-synthesis of Dysarthric Speech</a>&quot;, Proceedings of 5th ISCA
	Workshop on Speech Synthesis, June 2004.</p>
	<li/>
<p style="margin-bottom: 0in">J. Hosom, A. Kain, T. Mishra, J.
	van Santen, M. Fried-Oken, J. Staehely, &quot;<a href="pub/Hosom2003-ICASSP-Dysarthria.pdf">Intelligibility
	of modifications to dysarthric speech</a>&quot;, Proceedings of
	ICASSP, May 2003.</p>
</ul>
<h3 class="western">Text-to-Speech Synthesis (TTS)</h3>
<p style="margin-bottom: 0.2in">Text-to-Speech (TTS) Synthesis is the
process of generating human speech artificially from textual input.
Although TTS systems are becoming more and more commonplace, many
challenges remain to produce natural-sounding, meaningful output. We
have created algorithms that significantly reduce audible artifacts
in the synthesis output, that improve the naturalness of the
intonation contour, and that allow remarkable data compression of
acoustic inventories.</p>
<ul>
	<li/>
<p style="margin-bottom: 0in">M. Langarani, J. van Santen, S.
	Mohammadi, A. Kain, &quot;<a href="pub/Langarani2015-Interspeech-Intonation.pdf">Data-driven
	Foot-based Intonation Generator for Text-to-Speech Synthesis</a>&quot;,
	Interspeech, 2015.</p>
	<li/>
<p style="margin-bottom: 0in">A. Kain and T. Leen,
	&quot;<a href="pub/Kain2010-SSW7-AIMLSF.pdf">Compression of Line
	Spectral Frequency Parameters using the Asynchronous Interpolation
	Model</a>&quot;, 7th ISCA Workshop on Speech Synthesis, September
	2010.</p>
	<li/>
<p style="margin-bottom: 0in">Q. Miao, A. Kain, J. van Santen,
	&quot;<a href="pub/Miao2009-Interspeech-PerceptualTDXF.pdf">Perceptual
	Cost Function for Cross-fading Based Concatenation</a>&quot;,
	Interspeech, 2009.</p>
	<li/>
<p style="margin-bottom: 0in">R. Moldover, A. Kain,
	&quot;<a href="pub/Moldover2009-ICASSP-LSFAIM">Compression of Line
	Spectral Frequency Parameters with Asynchronous Interpolation</a>&quot;,
	ICASSP, April 2009.</p>
	<li/>
<p style="margin-bottom: 0in">A. Kain, Q. Miao, J. van Santen,
	&quot;<a href="pub/Kain2007-SSW6-Spectral%20Control.pdf">Spectral
	Control in Concatenative Speech Synthesis</a>&quot;, 6th ISCA
	Workshop on Speech Synthesis, August 2007.</p>
	<li/>
<p style="margin-bottom: 0in">A. Kain and J. van Santen,
	&quot;<a href="pub/Kain2007-SSW6-AIM.pdf">Unit-Selection
	Text-to-Speech Synthesis Using an Asynchronous Interpolation Model</a>&quot;,
	6th ISCA Workshop on Speech Synthesis, August 2007.</p>
	<li/>
<p style="margin-bottom: 0in">E. Klabbers, J. van Santen, A.
	Kain, &quot;<a href="pub/Klabbers2006-IEEE-Spectral%20Mismatch.pdf">The
	Contribution of Various Sources of Spectral Mismatch to Audible
	Discontinuities in a Diphone Database</a>&quot;, IEEE Transactions
	on Audio, Speech, and Language Processing Journal, Volume 15, Issue
	3, Pages 949-956, March 2007.</p>
	<li/>
<p style="margin-bottom: 0in">J. van Santen, A. Kain, E.
	Klabbers, and T. Mishra, &quot;<a href="pub/vanSanten2005-SPECOM-Multi-level%20Units.pdf">Synthesis
	of Prosody using Multi-level Unit Sequences</a>&quot;, Speech
	Communication Journal, Volume 46, Issues 3-4, Pages 365-375, July
	2005.</p>
	<li/>
<p style="margin-bottom: 0in">J. van Santen, A. Kain, and E.
	Klabbers, &quot;<a href="pub/vanSanten2004-SpeechProsody-Synthesis%20by%20Recombination.pdf">Synthesis
	by Recombination of Segmental and Prosodic Information</a>&quot;,
	Speech Prosody 2004, March 2004.</p>
	<li/>
<p style="margin-bottom: 0in">A. Kain and J. van Santen, &quot;<a href="pub/Kain2003-Eurospeech-AIM.pdf">A
	speech model of acoustic inventories based on asynchronous
	interpolation</a>&quot;, EUROSPEECH, Pages 329-332, August 2003.</p>
	<li/>
<p style="margin-bottom: 0in">J. van Santen, L. Black, G.
	Cohen, A. Kain, E. Klabbers, T. Mishra, J. de Villiers, X. Niu,
	&quot;<a href="pub/vanSanten2003-Eurospeech-Expressive.pdf">Applications
	of computer generated expressive speech for communication
	disorders</a>&quot;, EUROSPEECH, Pages 1657-1660, August 2003.</p>
	<li/>
<p style="margin-bottom: 0in">A. Kain and J. van Santen,
	&quot;<a href="pub/Kain2002-IEEE-AIM.pdf">Compression of Acoustic
	Inventories using Asynchronous Interpolation</a>&quot;, IEEE
	Workshop on Speech Synthesis, Pages 83-86, September 2002.</p>
	<li/>
<p style="margin-bottom: 0in">J. van Santen, J. Wouters, and
	A. Kain, &quot;<a href="pub/vanSanten2002-IEEE-Tribute.pdf">Modification
	of Speech: A Tribute to Mike Macon</a>&quot;, IEEE Workshop on
	Speech Synthesis, September 2002.</p>
	<li/>
<p style="margin-bottom: 0in">A. Kain and Y. Stylianou,
	&quot;<a href="pub/Kain2000-ICASSP-Spectral%20Pitch.pdf">Stochastic
	Modeling of Spectral Adjustment for High Quality Pitch
	Modification</a>&quot;, ICASSP, June 2000, vol. 2, pp. 949-952.</p>
</ul>
<h3 class="western">Voice Conversion</h3>
<p style="margin-bottom: 0.2in">Voice Conversion modifies a source
speaker's utterance to sound as if a target speaker had spoken it.
Its uses include entertainment and security applications, and, most
importantly, adaptation of text-to-speech systems' voices to new
speakers, especially benefitting individuals who depend on
speech-generating devices for communication. Despite continuous
research in the field, speech quality and mimic accuracy is still
insufficient for everyday usage. We have advanced the state of the
art by researching novel approaches, including the use of
joint-density Gaussian mixture models and semi-supervised learning
with deep autoencoders and deep neural networks.</p>
<ul>
	<li/>
<p style="margin-bottom: 0in">S. Mohammadi, A. Kain, “<a href="pub/Mohammadi2017-SPECOM-Overview.pdf">An
	Overview of Voice Conversion Systems</a>”, SPECOM, 2017.</p>
	<li/>
<p style="margin-bottom: 0in">S. Mohammadi, A. Kain, &quot;<a href="pub/Mohammadi2016-Interspeech-SJAE%20Map.pdf">A
	Voice Conversion Mapping Function based on a Stacked
	Joint-Autoencoder</a>&quot;, Interspeech, 2016.</p>
	<li/>
<p style="margin-bottom: 0in">S. Mohammadi, A. Kain,
	&quot;<a href="pub/Mohammadi2015-Interspeech-semisupervised.pdf">Semi-supervised
	Training of a Voice Conversion Mapping Function using a
	Joint-Autoencoder</a>&quot;, Interspeech, 2015.</p>
	<li/>
<p style="margin-bottom: 0in">S. Mohammadi, A. Kain, &quot;<a href="pub/Mohammadi2014-SLT-DNN.pdf">Voice
	conversion using Deep Neural Networks with speaker-independent
	pre-training</a>&quot;, IEEE Spoken Language Technology Workshop
	(SLT), 2014.</p>
	<li/>
<p style="margin-bottom: 0in">S. Mohammadi, A. Kain,
	&quot;<a href="pub/Mohammadi2013-ICASSP-Transmutative.pdf">Transmutative
	Voice Conversion</a>&quot;, ICASSP, 2013.</p>
	<li/>
<p style="margin-bottom: 0in">E. Morley, E. Klabbers, J. van
	Santen, A. Kain, S. Mohammadi, &quot;<a href="pub/Morley2012-Interspeech-F0%20Speaker%20ID.pdf">Synthetic
	F0 can Effectively Convey Speaker ID in Delexicalized Speech</a>&quot;,
	Interspeech, 2012.</p>
	<li/>
<p style="margin-bottom: 0in">E. Morley, J. van Santen, E.
	Klabbers, A. Kain, &quot;<a href="pub/Morley2011-ICASSP-F0.pdf">F0
	Range and Peak Alignment across Speakers and Emotions</a>&quot;,
	ICASSP, 2011.</p>
	<li/>
<p style="margin-bottom: 0in">E. Klabbers, A. Kain, and J. van
	Santen, &quot;<a href="pub/Klabbers2010-IS-SGD.pdf">Evaluation of
	speaker mimic technology for personalizing SGD voices</a>&quot;,
	Interspeech, 2010.</p>
	<li/>
<p style="margin-bottom: 0in">H. Duxans, A. Bonafonte, A.
	Kain, and J. van Santen, &quot;<a href="pub/Duxans2004-ICSLP-Dynamic.pdf">Including
	Dynamic and Phonetic Information in Voice Conversion Systems</a>&quot;,
	ICSLP, October 2004.</p>
	<li/>
<p style="margin-bottom: 0in">A. Kain, &quot;<a href="pub/Kain2001-Thesis-Voice%20Conversion.pdf">High
	Resolution Voice Transformation</a>&quot;, Ph.D. thesis, OGI School
	of Science &amp; Engineering at Oregon Health &amp; Science
	University, 2001. The data used in this thesis are available from
	the Linguistic Data Consortium as the <a href="http://www.ldc.upenn.edu/Catalog/CatalogEntry.jsp?catalogId=LDC2006S01">VOICES
	Corpus</a>.</p>
	<li/>
<p style="margin-bottom: 0in">A. Kain and M. Macon, &quot;<a href="pub/Kain2001-ICASSP-Residual%20Prediction.pdf">Design
	and Evaluation of a Voice Conversion Algorithm based on Spectral
	Envelope Mapping and Residual Prediction</a>&quot;, ICASSP, May
	2001.</p>
	<li/>
<p style="margin-bottom: 0in">A. Kain and M. Macon,
	&quot;<a href="pub/Kain1998-SSW3-Personalizing.pdf">Personalizing a
	speech synthesizer by voice adaptation</a>&quot;, 3rd ESCA/COCOSDA
	International Speech Synthesis Workshop, November 1998, pp. 225-230.</p>
	<li/>
<p style="margin-bottom: 0in">A. Kain and M. Macon,
	&quot;<a href="pub/Kain1998-ICSLP-Sparse.pdf">Text-to-speech voice
	adaptation from sparse training data</a>&quot;, ICSLP, November
	1998, vol.7, pp. 2847-50.</p>
	<li/>
<p style="margin-bottom: 0in">A. Kain and M. Macon, &quot;<a href="pub/Kain1998-ICASSP-Voice%20Conversion.pdf">Spectral
	Voice Conversion for Text-to-Speech Synthesis</a>&quot;, ICASSP, May
	1998, vol. 1, pp. 285-288.</p>
</ul>
<h3 class="western">Coarticulation</h3>
<p style="margin-bottom: 0.2in">Coarticulation refers to the
phenomenon in which a conceptually isolated speech sound becomes more
similar to a preceding or following speech sound. Modeling
coarticulation in speech has been largely limited to short sequences
and/or limited phonetic context. We introduce a methodology for
modeling both formant frequencies and bandwidth in continuous speech.
Applications of such a model include improved formant tracking,
characterization of conversational vs. clear speech, and detection of
typical vs. disordered speech.</p>
<ul>
	<li/>
<p style="margin-bottom: 0in">B. Bush, A. Kain, &quot;<a href="pub/Bush2014-IS-Cont.pdf">Modeling
	Coarticulation in Continuous Speech</a>&quot;, Interspeech 2014.</p>
	<li/>
<p style="margin-bottom: 0in">A. Amano-Kusumoto, J.-P Hosom,
	A. Kain, J. Aronoff, &quot;<a href="http://authors.elsevier.com/sd/article/S0167639313001684">Determining
	the relevance of different aspects of formant contours to
	intelligibility</a>&quot;, Speech Communication, Volume 59, April
	2014.</p>
	<li/>
<p style="margin-bottom: 0in">B. Bush, A. Kain, &quot;<a href="pub/Bush2013-ICASSP-CoarticulationBF.pdf">Estimating
	Phoneme Formant Targets and Coarticulation Parameters of
	Conversational and Clear Speech</a>&quot;, ICASSP, 2013.</p>
	<li/>
<p style="margin-bottom: 0in">B. Bush, J.-P. Hosom, A. Kain,
	and A. Amano-Kusumoto, &quot;<a href="pub/Bush2011-IS-GA.pdf">Using
	a genetic algorithm to estimate parameters of a coarticulation
	model</a>&quot;, Interspeech, 2011.</p>
	<li/>
<p style="margin-bottom: 0in">A. Amano-Kusumoto, J.-P. Hosom,
	and A. Kain, &quot;<a href="pub/Amano2010-IS-SpeakingStyle.pdf">Speaking
	style dependency of formant targets</a>&quot;, Interspeech, 2010.</p>
</ul>
<h3 class="western">Sleep Apnea</h3>
<ul>
	<li/>
<p style="margin-bottom: 0in">B. Snider and A. Kain,
	&quot;<a href="pub/Snider2016-ICASSP-ClassificationApnea.pdf">Classification
	of Respiratory Effort and Disordered Breathing during Sleep from
	Audio and Pulse Oximetry Signals</a>&quot;, ICASSP, 2016.</p>
	<li/>
<p style="margin-bottom: 0in">B. Snider and A. Kain,
	&quot;<a href="pub/Snider2013-ICASSP-BreathingHMM.pdf">Automatic
	Classification of Breathing Sounds during Sleep</a>&quot;, ICASSP,
	2013.</p>
</ul>
<h3 class="western">Miscellaneous</h3>
<ul>
	<li/>
<p style="margin-bottom: 0in">S. Dudy, M. Asgari, and A. Kain,
	&quot;<a href="pub/Dudy2015-EMBC-Pronunciation">Pronunciation
	Analysis for Children with Speech Sound Disorders</a>&quot;, IEEE
	Engineering in Medicine and Biology society (EMBC), Milan, 2015.
	(PMC4710861)</p>
	<li/>
<p style="margin-bottom: 0in">J. House, A. Kain, and J. Hines,
	&quot;<a href="pub/House2000-GECCO-ESP.pdf">ESP - Metaphor for
	learning: an evolutionary algorithm</a>&quot;, GECCO 2000, Las
	Vegas, NV.</p>
	<li/>
<p style="margin-bottom: 0in">S. Sutton, R. Cole, J. de
	Villiers, J. Schalkwyk, P. Vermeulen, M. Macon, Y. Yan, E. Kaiser,
	B. Rundle, K. Shobaki, P. Hosom, A. Kain, J. Wouters, D. Massaro, M.
	Cohen, &quot;<a href="pub/Sutton1998-ICSLP-CSLU%20Toolkit.pdf">Universal
	Speech Tools: The CSLU Toolkit</a>&quot;, ICSLP, November 1998, vol.
	7, pp. 3221-24.</p>
	<li/>
<p>N. Malayath, H. Hermansky, A. Kain and R. Carlson,
	&quot;<a href="pub/Malayath1997-Eurospeech-SI%20by%20PCA.pdf">Speaker-independent
	Feature Extraction by Oriented Principal Component Analysis</a>&quot;,
	EUROSPEECH, 1997.</p>
</ul>
<h2 class="western">Abstracts</h2>
<ul>
	<li/>
<p style="margin-bottom: 0in">A. Kain, &quot;Speech
	transformation: Increasing intelligibility and changing speakers&quot;,
	Journal of the Acoustical Society of America, 126(4):2205 (2009).</p>
	<li/>
<p>J.P. Hosom, A. Kain, and B. Bush, &quot;<a href="http://scitation.aip.org/content/asa/journal/jasa/130/4/10.1121/1.3654650">Towards
	the recovery of targets from coarticulated speech for automatic
	speech recognition</a>&quot;, The Journal of the Acoustical Society
	of America, 130(4), pages 2407-2407, 2011.</p>
</ul>
<h2 class="western">Patents</h2>
<ul>
	<li/>
<p style="margin-bottom: 0in">J. van Santen and A. Kain, OHSU.
	<i>System and Method for Compressing Concatenative Acoustic
	Inventories for Speech Synthesis.</i></p>
	<li/>
<p>A. Kain and Y. Stylianou, AT&amp;T Research Laboratories.
	<i>Stochastic Modeling Of Spectral Adjustment For High Quality Pitch
	Modification.</i></p>
</ul>
<h2 class="western">Technical Reports</h2>
<ul>
	<li/>
<p style="margin-bottom: 0in">B. R. Snider and A. Kain,
	&quot;<a href="pub/CSLU-2012-001.pdf">Adaptive Reduction of Additive
	Noise from Sleep Breathing Sounds</a>&quot;, CSLU-2012-001.&nbsp;</p>
	<li/>
<p style="margin-bottom: 0in">A. Kain, J.-P. Hosom, S. H.
	Ferguson, B. Bush, &quot;<a href="pub/CSLU-11-003.pdf">Creating a
	speech corpus with semi-spontaneous, parallel conversational and
	clear speech</a>&quot;, CSLU-11-003.&nbsp;</p>
	<li/>
<p>A. Amano-Kusumoto and J.-P. Hosom, &quot;<a href="pub/CSLU-11-001.pdf">A
	review of research on speech intelligibility and correlations with
	acoustic features</a>&quot;, CSLU-11-001.&nbsp;</p>
</ul>
<h2 class="western">Audio Demos</h2>
<ul>
	<li/>
<p style="margin-bottom: 0in">Quantitative assessment and
	transformation of clear and conversational speech, with the aim of
	advancing hearing-aid performance (without extra noise:
	<a href="demo/conv-clear/H-conv.wav">conversational</a>, <a href="demo/conv-clear/H-clearPros+convSpec.wav">clear
	prosody and conversational spectrum</a>, <a href="demo/conv-clear/H-convPros+clearSpec.wav">conversational
	prosody and clear spectrum</a>, <a href="demo/conv-clear/H-clear.wav">clear</a>;
	with multi-talker background noise: <a href="demo/conv-clear/S-conv.wav">conversational</a>,
	<a href="demo/conv-clear/S-clearPros+convSpec.wav">clear prosody and
	conversational spectrum</a>, <a href="demo/conv-clear/S-convPros+clearSpec.wav">conversational
	prosody and clear spectrum</a>, <a href="demo/conv-clear/S-clear.wav">clear</a>)</p>
	<li/>
<p style="margin-bottom: 0in">Transformation of aphonic speech
	to improve intelligibility and acceptability ( <a href="demo/aphonia/axk-aph.wav">aphonic
	speech</a>, <a href="demo/aphonia/axk-mod.wav">transformation</a>)</p>
	<li/>
<p style="margin-bottom: 0in">Transformation of dysarthric
	speech to improve intelligibility and perceived voice quality (
	<a href="demo/dysarthria/40_dys_ae_shack_4020.wav">dysarthric
	speech</a>, <a href="demo/dysarthria/40_dys-map-gen_ae_shack_4020.wav">transformation</a>)</p>
	<li/>
<p style="margin-bottom: 0in">Increasing spectral control in
	concatenative synthesizers to eliminate concatenation errors
	(<a href="demo/hybrid-TTS/original.wav">baseline</a>, <a href="demo/hybrid-TTS/ffxf+sbxf+tdxf.wav">formant
	+ spectral-band + time-domain crossfading</a>)</p>
	<li/>
<p style="margin-bottom: 0in">Representing acoustic
	inventories of Text-to-Speech systems with an asynchronous
	interpolation model, allowing high rates of compression, elimination
	of concatenation errors, and speaker transformation (compression:
	<a href="demo/aim/as-655.wav">original</a>, <a href="demo/aim/as-655-aim3400bps.wav">compression
	with AIM coder @ 3.4kbps</a>, <a href="demo/aim/as-655-speex3400bps.wav">compression
	with speex coder @ 3.4kbps for comparison</a>; speaker
	transformation: <a href="demo/aim/as2jcs-655.wav">transformation-1</a>,
	<a href="demo/aim/as2jgl-655.wav">transformation-2</a>,
	<a href="demo/aim/as2mam-655.wav">transformation-3</a>,
	<a href="demo/aim/as2mwm-655.wav">transformation-4</a>,
	<a href="demo/aim/as2scs-655.wav">transformation-5</a>)</p>
	<li/>
<p style="margin-bottom: 0in">Improving the accuracy and
	quality of speaker transformation systems and designing speaker
	recognizability perceptual tests (transformation of natural speech:
	<a href="demo/speaker-transformation/source.wav">source</a>,
	<a href="demo/speaker-transformation/transformation.wav">transformation</a>,
	<a href="demo/speaker-transformation/target.wav">target</a>;
	transformation of TTS synthesis voices: <a href="demo/TTS-adaptation/source.wav">source</a>,
	<a href="demo/TTS-adaptation/transformation.wav">transformation</a>,
	<a href="demo/TTS-adaptation/target.wav">target</a>)</p>
	<li/>
<p style="margin-bottom: 0in">Multi-purpose speech
	modification algorithms (<a href="demo/modification/original.wav">original</a>,
	<a href="demo/modification/resynthesis.wav">resynthesis</a>, <a href="demo/modification/slow-300.wav">slow
	to 300%</a>, <a href="demo/modification/fast-50.wav">speed-up to
	50%</a>, <a href="demo/modification/down-50.wav">lower pitch to 50%</a>,
	<a href="demo/modification/up-200.wav">raise pitch to 200%</a>,
	<a href="demo/modification/form-80.wav">scale formants to 80%</a>,
	<a href="demo/modification/form-120.wav">scale formants to 120%</a>,
	<a href="demo/modification/mimic-child.wav">mimic child</a>, <a href="demo/modification/mimic-man.wav">mimic
	man</a>)</p>
	<li/>
<p>Singing synthesis 1999 (<a href="demo/singing/TheSearchIsOver.wav">&quot;The
	Search is Over&quot;</a>)</p>
</ul>
</body>
</html>
